{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/miniconda3/envs/tfl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/akhare/miniconda3/envs/tfl/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/home/akhare/miniconda3/envs/tfl/lib/python3.11/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "import sys\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "from torch.utils.data import *\n",
    "from transformers import *\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from models import *\n",
    "from logic import *\n",
    "from my_datasets import *\n",
    "\n",
    "# from utils import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"transformer_friends\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" # log all model checkpoints\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n, r = 5, 8\n",
    "# n, r = 20, 5\n",
    "n, r = 5, 8\n",
    "ap, bp, tp, sp = 0.2, 0.2, 0.4, 0.1\n",
    "\n",
    "nars = 3\n",
    "\n",
    "train_len = 2500\n",
    "test_len = 500\n",
    "num_epochs = 10\n",
    "seed = 42\n",
    "# test_is_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AutoRegKStepsEmbedsDataset(\n",
    "    num_rules = r,\n",
    "    num_vars = n,\n",
    "    num_steps = nars,\n",
    "    ante_prob = ap,\n",
    "    conseq_prob = bp,\n",
    "    state_prob = sp,\n",
    "    dataset_len = train_len,\n",
    "    seed = seed)\n",
    "\n",
    "eval_dataset = AutoRegKStepsEmbedsDataset(\n",
    "    num_rules = r,\n",
    "    num_vars = n,\n",
    "    num_steps = nars,\n",
    "    ante_prob = ap,\n",
    "    conseq_prob = bp,\n",
    "    state_prob = sp,\n",
    "    dataset_len = test_len,\n",
    "    seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rules': tensor([[0, 1, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 1, 1, 0],\n",
       "         [0, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 1, 1, 1, 1, 0]]),\n",
       " 'state': tensor([0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([[0, 0, 1, 0, 0],\n",
       "         [0, 0, 1, 0, 1],\n",
       "         [0, 0, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_rule(rule, var_sep_token):\n",
    "    \"\"\"\n",
    "    Create a rule of the form xi , xj , ... -> xa\n",
    "    from a one-hot vector of [<ants>, <cons>]\n",
    "    \"\"\"\n",
    "\n",
    "    n_vars = len(rule) // 2\n",
    "    ants = [f\"x{i}\" for i in range(n_vars) if rule[i]]\n",
    "    cons = [f\"x{i}\" for i in range(n_vars) if rule[n_vars+i]]\n",
    "    if len(ants) < 1:\n",
    "        ants = [\"empty\"]\n",
    "    if len(cons) < 1:\n",
    "        cons = [\"empty\"]\n",
    "    rule = var_sep_token.join(ants) + \" -> \" + var_sep_token.join(cons)\n",
    "    return rule\n",
    "\n",
    "def get_string_rep_replace(dataset_item):\n",
    "    \"\"\"\n",
    "    Returns a string of the form:\n",
    "    [RULES_START] [RULE_START] ... [RULE_END] ... [RULES_END]\n",
    "    [CURRENT_STATE_START] ... [CURRENT_STATE_END]\n",
    "    [NEXT_STATE_START] ... [NEXT_STATE_END]\n",
    "    \"\"\"\n",
    "\n",
    "    var_sep_token = \" , \"\n",
    "    rules_start = \"[RULES_START]\"\n",
    "    rules_end = \"[RULES_END]\"\n",
    "    rule_start = \"[RULE_START]\"\n",
    "    rule_end = \"[RULE_END]\"\n",
    "    current_state_start = \"[CURRENT_STATE_START]\"\n",
    "    current_state_end = \"[CURRENT_STATE_END]\"\n",
    "    next_state_start = \"[NEXT_STATE_START]\"\n",
    "    next_state_end = \"[NEXT_STATE_END]\"\n",
    "\n",
    "    rules = dataset_item[\"rules\"]\n",
    "    current_state = dataset_item[\"state\"]\n",
    "    next_state = dataset_item[\"labels\"][0]\n",
    "\n",
    "    n_vars = len(current_state)\n",
    "\n",
    "    rule_strs = [rule_start + \" \" + stringify_rule(rule, var_sep_token) + \" \" + rule_end for rule in rules]\n",
    "    current_state_str = var_sep_token.join([f\"x{i}\" for i in range(n_vars) if current_state[i]])\n",
    "    current_state_str = current_state_start + \" \" + current_state_str + \" \" + current_state_end\n",
    "    rules_str = rules_start + \" \" + \" \".join(rule_strs) + \" \" + rules_end\n",
    "\n",
    "    next_state_str = var_sep_token.join([f\"x{i}\" for i in range(n_vars) if next_state[i]])\n",
    "    return {\n",
    "        \"prompt\": rules_str + \" \" + current_state_str + \" \" + next_state_start,\n",
    "        \"target\": \" \" + next_state_str + \" \" + next_state_end,\n",
    "        \"stop\": next_state_end\n",
    "    }\n",
    "    return rules_str + \" \" + current_state_str + \" \" + next_state_start, {\"stop\": next_state_end}\n",
    "\n",
    "def get_string_rep_append(dataset_item):\n",
    "    \"\"\"\n",
    "    Returns a string of the form:\n",
    "    [RULES_START] [RULE_START] ... [RULE_END] ... [RULES_END]\n",
    "    [STATES_START] [STATE_START] ... [STATE_END] ... [STATES_END]\n",
    "    \"\"\"\n",
    "\n",
    "    var_sep_token = \" , \"\n",
    "    rules_start = \"[RULES_START]\"\n",
    "    rules_end = \"[RULES_END]\"\n",
    "    rule_start = \"[RULE_START]\"\n",
    "    rule_end = \"[RULE_END]\"\n",
    "    states_start = \"[STATES_START]\"\n",
    "    states_end = \"[STATES_END]\"\n",
    "    state_start = \"[STATE_START]\"\n",
    "    state_end = \"[STATE_END]\"\n",
    "\n",
    "    rules = dataset_item[\"rules\"]\n",
    "    state = dataset_item[\"state\"]\n",
    "    next_states = dataset_item[\"labels\"]\n",
    "\n",
    "    n_vars = len(state)\n",
    "\n",
    "    rule_strs = [rule_start + \" \" + stringify_rule(rule, var_sep_token) + \" \" + rule_end for rule in rules]\n",
    "    state_str = var_sep_token.join([f\"x{i}\" for i in range(n_vars) if state[i]])\n",
    "    state_str = state_start + \" \" + state_str + \" \" + state_end\n",
    "    rules_str = rules_start + \" \" + \" \".join(rule_strs) + \" \" + rules_end\n",
    "\n",
    "    next_state_strs = [var_sep_token.join([f\"x{i}\" for i in range(n_vars) if next_state[i]]) for next_state in next_states]\n",
    "    next_state_strs = [state_start + \" \" + next_state_str + \" \" + state_end for next_state_str in next_state_strs]\n",
    "    next_state_strs = \" \".join(next_state_strs)\n",
    "    # Remove the first state_start from the next state string\n",
    "    next_state_strs = next_state_strs[len(state_start)+1:]\n",
    "    return {\n",
    "        \"prompt\": rules_str + \" \" + states_start + \" \" + state_str + \" \" + state_start,\n",
    "        \"target\": \" \" + next_state_strs + \" \" + states_end,\n",
    "        \"stop\": states_end\n",
    "    }\n",
    "    return rules_str + \" \" + states_start + \" \" + state_str + \" \" + state_start, {\"stop\": states_end}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rules': tensor([[0, 1, 1, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 1, 1, 0],\n",
      "        [0, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 1, 1, 1, 1, 0]]), 'state': tensor([0, 0, 0, 0, 0]), 'labels': tensor([[0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 1, 1]])}\n",
      "{'prompt': '[RULES_START] [RULE_START] x1 , x2 , x4 -> empty [RULE_END] [RULE_START] x1 -> empty [RULE_END] [RULE_START] x2 -> x4 [RULE_END] [RULE_START] empty -> x2 [RULE_END] [RULE_START] x4 -> x2 , x3 [RULE_END] [RULE_START] x1 , x3 , x4 -> x1 [RULE_END] [RULE_START] empty -> x2 [RULE_END] [RULE_START] x0 -> x0 , x1 , x2 , x3 [RULE_END] [RULES_END] [CURRENT_STATE_START]  [CURRENT_STATE_END] [NEXT_STATE_START]', 'target': ' x2 [NEXT_STATE_END]', 'stop': '[NEXT_STATE_END]'}\n",
      "{'prompt': '[RULES_START] [RULE_START] x1 , x2 , x4 -> empty [RULE_END] [RULE_START] x1 -> empty [RULE_END] [RULE_START] x2 -> x4 [RULE_END] [RULE_START] empty -> x2 [RULE_END] [RULE_START] x4 -> x2 , x3 [RULE_END] [RULE_START] x1 , x3 , x4 -> x1 [RULE_END] [RULE_START] empty -> x2 [RULE_END] [RULE_START] x0 -> x0 , x1 , x2 , x3 [RULE_END] [RULES_END] [STATES_START] [STATE_START]  [STATE_END] [STATE_START]', 'target': ' x2 [STATE_END] [STATE_START] x2 , x4 [STATE_END] [STATE_START] x2 , x3 , x4 [STATE_END] [STATES_END]', 'stop': '[STATES_END]'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(get_string_rep_replace(train_dataset[0]))\n",
    "print(get_string_rep_append(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:03<00:00, 801.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 811.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create HuggingFace datasets for the append task\n",
    "\n",
    "print(\"Creating train dataset\")\n",
    "train_data = [get_string_rep_append(train_dataset[i]) for i in tqdm(range(len(train_dataset)))]\n",
    "train_hf_dataset = Dataset.from_dict({\n",
    "    # \"data\": [train_data[i]['prompt'] for i in range(len(train_data))],\n",
    "    # \"label\": [train_data[i]['target'] for i in range(len(train_data))],\n",
    "    \"data\": [train_data[i]['prompt'] + train_data[i]['target'] for i in range(len(train_data))],\n",
    "}).with_format(\"torch\")\n",
    "\n",
    "print(\"Creating test dataset\")\n",
    "test_data = [get_string_rep_append(eval_dataset[i]) for i in tqdm(range(len(eval_dataset)))]\n",
    "test_hf_dataset = Dataset.from_dict({\n",
    "    # \"data\": [test_data[i]['prompt'] for i in range(len(test_data))],\n",
    "    # \"label\": [test_data[i]['target'] for i in range(len(test_data))],\n",
    "    \"data\": [test_data[i]['prompt'] + test_data[i]['target'] for i in range(len(test_data))],\n",
    "}).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GPT-2 tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [00:00<00:00, 5437.59 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 6420.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(item):\n",
    "    return tokenizer(item[\"data\"], truncation=True)\n",
    "\n",
    "train_tokenized_dataset = train_hf_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized_dataset = test_hf_dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (input_ids: Optional[torch.LongTensor] = None, past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, attention_mask: Optional[torch.FloatTensor] = None, token_type_ids: Optional[torch.LongTensor] = None, position_ids: Optional[torch.LongTensor] = None, head_mask: Optional[torch.FloatTensor] = None, inputs_embeds: Optional[torch.FloatTensor] = None, encoder_hidden_states: Optional[torch.Tensor] = None, encoder_attention_mask: Optional[torch.FloatTensor] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None) -> Union[Tuple, transformers.modeling_outputs.CausalLMOutputWithCrossAttentions]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Check if all predictions match labels\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "    return acc\n",
    "    # return {\"Accuracy\" : acc[\"accuracy\"], \"Avg Ones\" : avg_ones}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2_string_auto_reg_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=5,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"gpt2-autoreg-str-tokenizer_default-vars_5-rules_8-train_2500-test_500\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=test_tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makhare\u001b[0m (\u001b[33mtransformer_friends\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/akhare/repos/tf_logic/notebooks/wandb/run-20231214_134426-m99jzz43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/transformer_friends/transformer_friends/runs/m99jzz43' target=\"_blank\">gpt2-autoreg-str-tokenizer_default-vars_5-rules_8-train_2500-test_500</a></strong> to <a href='https://wandb.ai/transformer_friends/transformer_friends' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/transformer_friends/transformer_friends' target=\"_blank\">https://wandb.ai/transformer_friends/transformer_friends</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/transformer_friends/transformer_friends/runs/m99jzz43' target=\"_blank\">https://wandb.ai/transformer_friends/transformer_friends/runs/m99jzz43</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacty of 10.75 GiB of which 229.62 MiB is free. Process 3504725 has 2.64 GiB memory in use. Including non-PyTorch memory, this process has 7.88 GiB memory in use. Of the allocated memory 5.37 GiB is allocated by PyTorch, and 1.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bash05.seas.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/transformers/trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2773\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2776\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2779\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/transformers/trainer.py:2801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2799\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2802\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[1;32m    185\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[0;32m--> 186\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgather(outputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:203\u001b[0m, in \u001b[0;36mDataParallel.gather\u001b[0;34m(self, outputs, output_device)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgather\u001b[39m(\u001b[39mself\u001b[39m, outputs: Any, output_device: Union[\u001b[39mint\u001b[39m, torch\u001b[39m.\u001b[39mdevice]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m gather(outputs, output_device, dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/parallel/scatter_gather.py:105\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(outputs, target_device, dim)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m# Recursive function calls like this create reference cycles.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m# Setting the function to None clears the refcycle.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     res \u001b[39m=\u001b[39m gather_map(outputs)\n\u001b[1;32m    106\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     gather_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/parallel/scatter_gather.py:96\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m outputs):\n\u001b[1;32m     95\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAll dicts must have the same number of keys\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(out)((k, gather_map([d[k] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m outputs]))\n\u001b[1;32m     97\u001b[0m                      \u001b[39mfor\u001b[39;49;00m k \u001b[39min\u001b[39;49;00m out)\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m     99\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)\u001b[39m.\u001b[39m_make(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m<string>:9\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, loss, logits, past_key_values, hidden_states, attentions, cross_attentions)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/transformers/utils/generic.py:362\u001b[0m, in \u001b[0;36mModelOutput.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m# if we provided an iterator as first field and the iterator is a (key, value) iterator\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m# set the associated fields\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39mif\u001b[39;00m first_field_iterator:\n\u001b[0;32m--> 362\u001b[0m     \u001b[39mfor\u001b[39;49;00m idx, element \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(iterator):\n\u001b[1;32m    363\u001b[0m         \u001b[39mif\u001b[39;49;00m (\n\u001b[1;32m    364\u001b[0m             \u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(element, (\u001b[39mlist\u001b[39;49m, \u001b[39mtuple\u001b[39;49m))\n\u001b[1;32m    365\u001b[0m             \u001b[39mor\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(element) \u001b[39m==\u001b[39;49m \u001b[39m2\u001b[39;49m\n\u001b[1;32m    366\u001b[0m             \u001b[39mor\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(element[\u001b[39m0\u001b[39;49m], \u001b[39mstr\u001b[39;49m)\n\u001b[1;32m    367\u001b[0m         ):\n\u001b[1;32m    368\u001b[0m             \u001b[39mif\u001b[39;49;00m idx \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m:\n\u001b[1;32m    369\u001b[0m                 \u001b[39m# If we do not have an iterator of key/values, set it as attribute\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/parallel/scatter_gather.py:96\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m outputs):\n\u001b[1;32m     95\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAll dicts must have the same number of keys\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)((k, gather_map([d[k] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m outputs]))\n\u001b[1;32m     97\u001b[0m                      \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m out)\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m     99\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)\u001b[39m.\u001b[39m_make(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/parallel/scatter_gather.py:90\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     88\u001b[0m out \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m Gather\u001b[39m.\u001b[39;49mapply(target_device, dim, \u001b[39m*\u001b[39;49moutputs)\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:75\u001b[0m, in \u001b[0;36mGather.forward\u001b[0;34m(ctx, target_device, dim, *inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     ctx\u001b[39m.\u001b[39munsqueezed_scalar \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     74\u001b[0m ctx\u001b[39m.\u001b[39minput_sizes \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(i\u001b[39m.\u001b[39msize(ctx\u001b[39m.\u001b[39mdim) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m inputs)\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m comm\u001b[39m.\u001b[39;49mgather(inputs, ctx\u001b[39m.\u001b[39;49mdim, ctx\u001b[39m.\u001b[39;49mtarget_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfl/lib/python3.11/site-packages/torch/nn/parallel/comm.py:231\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(tensors, dim, destination, out)\u001b[0m\n\u001b[1;32m    227\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    228\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mUsing -1 to represent CPU tensor is deprecated. Please use a \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    229\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mdevice object or string instead, e.g., \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    230\u001b[0m     destination \u001b[39m=\u001b[39m _get_device_index(destination, allow_cpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, optional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_gather(tensors, dim, destination)\n\u001b[1;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m destination \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacty of 10.75 GiB of which 229.62 MiB is free. Process 3504725 has 2.64 GiB memory in use. Including non-PyTorch memory, this process has 7.88 GiB memory in use. Of the allocated memory 5.37 GiB is allocated by PyTorch, and 1.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt2-autoreg-str-tokenizer_default-vars_5-rules_8-train_2500-test_500</strong> at: <a href='https://wandb.ai/transformer_friends/transformer_friends/runs/o7vsyso6' target=\"_blank\">https://wandb.ai/transformer_friends/transformer_friends/runs/o7vsyso6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231214_133052-o7vsyso6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
