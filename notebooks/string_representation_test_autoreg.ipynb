{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "import sys\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "from torch.utils.data import *\n",
    "from transformers import *\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from models import *\n",
    "from my_datasets import *\n",
    "\n",
    "# from utils import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"transformer_friends\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" # log all model checkpoints\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n, r = 5, 8\n",
    "# n, r = 20, 5\n",
    "# n, r = 5, 8\n",
    "n, r = 10, 10\n",
    "ap, bp, tp, sp = 0.2, 0.2, 0.4, 0.1\n",
    "\n",
    "nars = 3\n",
    "\n",
    "train_len = 2500\n",
    "test_len = 500\n",
    "num_epochs = 15\n",
    "seed = 42\n",
    "# test_is_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AutoRegKStepsEmbedsDataset(\n",
    "    num_rules = r,\n",
    "    num_vars = n,\n",
    "    num_steps = nars,\n",
    "    ante_prob = ap,\n",
    "    conseq_prob = bp,\n",
    "    state_prob = sp,\n",
    "    dataset_len = train_len,\n",
    "    seed = seed)\n",
    "\n",
    "eval_dataset = AutoRegKStepsEmbedsDataset(\n",
    "    num_rules = r,\n",
    "    num_vars = n,\n",
    "    num_steps = nars,\n",
    "    ante_prob = ap,\n",
    "    conseq_prob = bp,\n",
    "    state_prob = sp,\n",
    "    dataset_len = test_len,\n",
    "    seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rules': tensor([[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "         [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]),\n",
       " 'state': tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0]),\n",
       " 'labels': tensor([[1, 0, 1, 1, 0, 1, 0, 0, 1, 0],\n",
       "         [1, 0, 1, 1, 0, 1, 1, 0, 1, 0],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 0, 1, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_rule(rule, var_sep_token):\n",
    "    \"\"\"\n",
    "    Create a rule of the form xi , xj , ... -> xa\n",
    "    from a one-hot vector of [<ants>, <cons>]\n",
    "    \"\"\"\n",
    "\n",
    "    n_vars = len(rule) // 2\n",
    "    ants = [f\"x{i}\" for i in range(n_vars) if rule[i]]\n",
    "    cons = [f\"x{i}\" for i in range(n_vars) if rule[n_vars+i]]\n",
    "    if len(ants) < 1:\n",
    "        ants = [\"empty\"]\n",
    "    if len(cons) < 1:\n",
    "        cons = [\"empty\"]\n",
    "    rule = var_sep_token.join(ants) + \" -> \" + var_sep_token.join(cons)\n",
    "    return rule\n",
    "\n",
    "def get_string_rep_replace(dataset_item):\n",
    "    \"\"\"\n",
    "    Returns a string of the form:\n",
    "    [RULES_START] [RULE_START] ... [RULE_END] ... [RULES_END]\n",
    "    [CURRENT_STATE_START] ... [CURRENT_STATE_END]\n",
    "    [NEXT_STATE_START] ... [NEXT_STATE_END]\n",
    "    \"\"\"\n",
    "\n",
    "    var_sep_token = \" , \"\n",
    "    rules_start = \"[RULES_START]\"\n",
    "    rules_end = \"[RULES_END]\"\n",
    "    rule_start = \"[RULE_START]\"\n",
    "    rule_end = \"[RULE_END]\"\n",
    "    current_state_start = \"[CURRENT_STATE_START]\"\n",
    "    current_state_end = \"[CURRENT_STATE_END]\"\n",
    "    next_state_start = \"[NEXT_STATE_START]\"\n",
    "    next_state_end = \"[NEXT_STATE_END]\"\n",
    "\n",
    "    rules = dataset_item[\"rules\"]\n",
    "    current_state = dataset_item[\"state\"]\n",
    "    next_state = dataset_item[\"labels\"][0]\n",
    "\n",
    "    n_vars = len(current_state)\n",
    "\n",
    "    rule_strs = [rule_start + \" \" + stringify_rule(rule, var_sep_token) + \" \" + rule_end for rule in rules]\n",
    "    current_state_str = var_sep_token.join([f\"x{i}\" for i in range(n_vars) if current_state[i]])\n",
    "    current_state_str = current_state_start + \" \" + current_state_str + \" \" + current_state_end\n",
    "    rules_str = rules_start + \" \" + \" \".join(rule_strs) + \" \" + rules_end\n",
    "\n",
    "    next_state_str = var_sep_token.join([f\"x{i}\" for i in range(n_vars) if next_state[i]])\n",
    "    return {\n",
    "        \"prompt\": rules_str + \" \" + current_state_str + \" \" + next_state_start,\n",
    "        \"target\": \" \" + next_state_str + \" \" + next_state_end,\n",
    "        \"stop\": next_state_end\n",
    "    }\n",
    "    return rules_str + \" \" + current_state_str + \" \" + next_state_start, {\"stop\": next_state_end}\n",
    "\n",
    "def get_string_rep_append(dataset_item):\n",
    "    \"\"\"\n",
    "    Returns a string of the form:\n",
    "    [RULES_START] [RULE_START] ... [RULE_END] ... [RULES_END]\n",
    "    [STATES_START] [STATE_START] ... [STATE_END] ... [STATES_END]\n",
    "    \"\"\"\n",
    "\n",
    "    var_sep_token = \" , \"\n",
    "    rules_start = \"[RULES_START]\"\n",
    "    rules_end = \"[RULES_END]\"\n",
    "    rule_start = \"[RULE_START]\"\n",
    "    rule_end = \"[RULE_END]\"\n",
    "    states_start = \"[STATES_START]\"\n",
    "    states_end = \"[STATES_END]\"\n",
    "    state_start = \"[STATE_START]\"\n",
    "    state_end = \"[STATE_END]\"\n",
    "\n",
    "    rules = dataset_item[\"rules\"]\n",
    "    state = dataset_item[\"state\"]\n",
    "    next_states = dataset_item[\"labels\"]\n",
    "\n",
    "    n_vars = len(state)\n",
    "\n",
    "    rule_strs = [rule_start + \" \" + stringify_rule(rule, var_sep_token) + \" \" + rule_end for rule in rules]\n",
    "    state_str = var_sep_token.join([f\"x{i}\" for i in range(n_vars) if state[i]])\n",
    "    state_str = state_start + \" \" + state_str + \" \" + state_end\n",
    "    rules_str = rules_start + \" \" + \" \".join(rule_strs) + \" \" + rules_end\n",
    "\n",
    "    next_state_strs = [var_sep_token.join([f\"x{i}\" for i in range(n_vars) if next_state[i]]) for next_state in next_states]\n",
    "    next_state_strs = [state_start + \" \" + next_state_str + \" \" + state_end for next_state_str in next_state_strs]\n",
    "    next_state_strs = \" \".join(next_state_strs)\n",
    "    # Remove the first state_start from the next state string\n",
    "    next_state_strs = next_state_strs[len(state_start)+1:]\n",
    "    return {\n",
    "        \"prompt\": rules_str + \" \" + states_start + \" \" + state_str + \" \" + state_start,\n",
    "        \"target\": \" \" + next_state_strs + \" \" + states_end,\n",
    "        \"stop\": states_end\n",
    "    }\n",
    "    return rules_str + \" \" + states_start + \" \" + state_str + \" \" + state_start, {\"stop\": states_end}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rules': tensor([[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
      "        [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 'state': tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0]), 'labels': tensor([[1, 0, 1, 1, 0, 1, 0, 0, 1, 0],\n",
      "        [1, 0, 1, 1, 0, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0]])}\n",
      "{'prompt': '[RULES_START] [RULE_START] x0 , x6 -> x0 , x1 , x2 [RULE_END] [RULE_START] empty -> x0 , x2 [RULE_END] [RULE_START] x3 , x7 , x8 -> x2 , x4 , x5 , x7 , x8 [RULE_END] [RULE_START] x1 , x3 , x9 -> empty [RULE_END] [RULE_START] x1 , x2 , x3 , x5 -> x1 , x2 , x5 [RULE_END] [RULE_START] x3 , x4 , x5 , x6 , x7 -> x1 , x7 [RULE_END] [RULE_START] x1 , x4 , x7 -> empty [RULE_END] [RULE_START] empty -> x2 , x5 , x8 [RULE_END] [RULE_START] x3 , x4 , x8 -> x0 , x6 [RULE_END] [RULE_START] x0 , x2 -> x6 [RULE_END] [RULES_END] [CURRENT_STATE_START] x3 , x8 [CURRENT_STATE_END] [NEXT_STATE_START]', 'target': ' x0 , x2 , x3 , x5 , x8 [NEXT_STATE_END]', 'stop': '[NEXT_STATE_END]'}\n",
      "{'prompt': '[RULES_START] [RULE_START] x0 , x6 -> x0 , x1 , x2 [RULE_END] [RULE_START] empty -> x0 , x2 [RULE_END] [RULE_START] x3 , x7 , x8 -> x2 , x4 , x5 , x7 , x8 [RULE_END] [RULE_START] x1 , x3 , x9 -> empty [RULE_END] [RULE_START] x1 , x2 , x3 , x5 -> x1 , x2 , x5 [RULE_END] [RULE_START] x3 , x4 , x5 , x6 , x7 -> x1 , x7 [RULE_END] [RULE_START] x1 , x4 , x7 -> empty [RULE_END] [RULE_START] empty -> x2 , x5 , x8 [RULE_END] [RULE_START] x3 , x4 , x8 -> x0 , x6 [RULE_END] [RULE_START] x0 , x2 -> x6 [RULE_END] [RULES_END] [STATES_START] [STATE_START] x3 , x8 [STATE_END] [STATE_START]', 'target': ' x0 , x2 , x3 , x5 , x8 [STATE_END] [STATE_START] x0 , x2 , x3 , x5 , x6 , x8 [STATE_END] [STATE_START] x0 , x1 , x2 , x3 , x5 , x6 , x8 [STATE_END] [STATES_END]', 'stop': '[STATES_END]'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(get_string_rep_replace(train_dataset[0]))\n",
    "print(get_string_rep_append(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:02<00:00, 1094.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1106.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create HuggingFace datasets for the append task\n",
    "\n",
    "print(\"Creating train dataset\")\n",
    "train_data = [get_string_rep_append(train_dataset[i]) for i in tqdm(range(len(train_dataset)))]\n",
    "train_hf_dataset = Dataset.from_dict({\n",
    "    # \"data\": [train_data[i]['prompt'] for i in range(len(train_data))],\n",
    "    # \"label\": [train_data[i]['target'] for i in range(len(train_data))],\n",
    "    \"data\": [train_data[i]['prompt'] + train_data[i]['target'] for i in range(len(train_data))],\n",
    "}).with_format(\"torch\")\n",
    "\n",
    "print(\"Creating test dataset\")\n",
    "test_data = [get_string_rep_append(eval_dataset[i]) for i in tqdm(range(len(eval_dataset)))]\n",
    "test_hf_dataset = Dataset.from_dict({\n",
    "    # \"data\": [test_data[i]['prompt'] for i in range(len(test_data))],\n",
    "    # \"label\": [test_data[i]['target'] for i in range(len(test_data))],\n",
    "    \"data\": [test_data[i]['prompt'] + test_data[i]['target'] for i in range(len(test_data))],\n",
    "}).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GPT-2 tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "def tokenize_function(item):\n",
    "    return tokenizer(item[\"data\"], truncation=True)\n",
    "\n",
    "train_tokenized_dataset = train_hf_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized_dataset = test_hf_dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (input_ids: Optional[torch.LongTensor] = None, past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, attention_mask: Optional[torch.FloatTensor] = None, token_type_ids: Optional[torch.LongTensor] = None, position_ids: Optional[torch.LongTensor] = None, head_mask: Optional[torch.FloatTensor] = None, inputs_embeds: Optional[torch.FloatTensor] = None, encoder_hidden_states: Optional[torch.Tensor] = None, encoder_attention_mask: Optional[torch.FloatTensor] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None) -> Union[Tuple, transformers.modeling_outputs.CausalLMOutputWithCrossAttentions]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    print(eval_pred.label_ids)\n",
    "    print(eval_pred.predictions)\n",
    "    print(dict(eval_pred).keys())\n",
    "    # exit()\n",
    "    return {\"Accuracy\": 0}\n",
    "    # predictions, labels = eval_pred\n",
    "    # # Check if all predictions match labels\n",
    "    # acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "    # return acc\n",
    "    # # return {\"Accuracy\" : acc[\"accuracy\"], \"Avg Ones\" : avg_ones}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    # output_dir=\"gpt2_string_auto_reg_results\",\n",
    "    output_dir=\"gpt2_append_autoreg_str_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=5,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"gpt2-append-autoreg-str-tokenizer_default-vars_10-rules_10-train_2500-test_500\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=test_tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makhare\u001b[0m (\u001b[33mtransformer_friends\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/akhare/repos/tf_logic/notebooks/wandb/run-20231214_184038-arqgxoex</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/transformer_friends/transformer_friends/runs/arqgxoex' target=\"_blank\">gpt2-append-autoreg-str-tokenizer_default-vars_10-rules_10-train_2500-test_500</a></strong> to <a href='https://wandb.ai/transformer_friends/transformer_friends' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/transformer_friends/transformer_friends' target=\"_blank\">https://wandb.ai/transformer_friends/transformer_friends</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/transformer_friends/transformer_friends/runs/arqgxoex' target=\"_blank\">https://wandb.ai/transformer_friends/transformer_friends/runs/arqgxoex</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9474, 'learning_rate': 1.9936507936507938e-05, 'epoch': 0.05}\n",
      "{'loss': 0.7275, 'learning_rate': 1.9873015873015875e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5858, 'learning_rate': 1.980952380952381e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4999, 'learning_rate': 1.9746031746031748e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4405, 'learning_rate': 1.9682539682539684e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4126, 'learning_rate': 1.961904761904762e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3871, 'learning_rate': 1.9555555555555557e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3795, 'learning_rate': 1.9492063492063494e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3669, 'learning_rate': 1.942857142857143e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3575, 'learning_rate': 1.9365079365079367e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3565, 'learning_rate': 1.9301587301587303e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3485, 'learning_rate': 1.923809523809524e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3457, 'learning_rate': 1.9174603174603176e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3437, 'learning_rate': 1.9111111111111113e-05, 'epoch': 0.67}\n",
      "{'loss': 0.343, 'learning_rate': 1.904761904761905e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3408, 'learning_rate': 1.8984126984126986e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3409, 'learning_rate': 1.8920634920634923e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3389, 'learning_rate': 1.885714285714286e-05, 'epoch': 0.86}\n",
      "{'loss': 0.3323, 'learning_rate': 1.8793650793650796e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3323, 'learning_rate': 1.8730158730158732e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3346, 'learning_rate': 1.866666666666667e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.32636767625808716, 'eval_runtime': 4.7729, 'eval_samples_per_second': 104.758, 'eval_steps_per_second': 4.4, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3335, 'learning_rate': 1.8603174603174605e-05, 'epoch': 1.05}\n",
      "{'loss': 0.3314, 'learning_rate': 1.853968253968254e-05, 'epoch': 1.1}\n",
      "{'loss': 0.333, 'learning_rate': 1.8476190476190478e-05, 'epoch': 1.14}\n",
      "{'loss': 0.3286, 'learning_rate': 1.8412698412698415e-05, 'epoch': 1.19}\n",
      "{'loss': 0.3298, 'learning_rate': 1.834920634920635e-05, 'epoch': 1.24}\n",
      "{'loss': 0.3302, 'learning_rate': 1.8285714285714288e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3251, 'learning_rate': 1.8222222222222224e-05, 'epoch': 1.33}\n",
      "{'loss': 0.3242, 'learning_rate': 1.815873015873016e-05, 'epoch': 1.38}\n",
      "{'loss': 0.3245, 'learning_rate': 1.8095238095238097e-05, 'epoch': 1.43}\n",
      "{'loss': 0.3242, 'learning_rate': 1.8031746031746034e-05, 'epoch': 1.48}\n",
      "{'loss': 0.3232, 'learning_rate': 1.796825396825397e-05, 'epoch': 1.52}\n",
      "{'loss': 0.32, 'learning_rate': 1.7904761904761907e-05, 'epoch': 1.57}\n",
      "{'loss': 0.3247, 'learning_rate': 1.7841269841269843e-05, 'epoch': 1.62}\n",
      "{'loss': 0.3186, 'learning_rate': 1.7777777777777777e-05, 'epoch': 1.67}\n",
      "{'loss': 0.3201, 'learning_rate': 1.7714285714285717e-05, 'epoch': 1.71}\n",
      "{'loss': 0.3164, 'learning_rate': 1.7650793650793653e-05, 'epoch': 1.76}\n",
      "{'loss': 0.3196, 'learning_rate': 1.758730158730159e-05, 'epoch': 1.81}\n",
      "{'loss': 0.3174, 'learning_rate': 1.7523809523809526e-05, 'epoch': 1.86}\n",
      "{'loss': 0.3164, 'learning_rate': 1.7460317460317463e-05, 'epoch': 1.9}\n",
      "{'loss': 0.3169, 'learning_rate': 1.73968253968254e-05, 'epoch': 1.95}\n",
      "{'loss': 0.312, 'learning_rate': 1.7333333333333336e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3220694661140442, 'eval_runtime': 4.7735, 'eval_samples_per_second': 104.746, 'eval_steps_per_second': 4.399, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3148, 'learning_rate': 1.7269841269841272e-05, 'epoch': 2.05}\n",
      "{'loss': 0.3122, 'learning_rate': 1.720634920634921e-05, 'epoch': 2.1}\n",
      "{'loss': 0.3178, 'learning_rate': 1.7142857142857142e-05, 'epoch': 2.14}\n",
      "{'loss': 0.3161, 'learning_rate': 1.707936507936508e-05, 'epoch': 2.19}\n",
      "{'loss': 0.3155, 'learning_rate': 1.7015873015873018e-05, 'epoch': 2.24}\n",
      "{'loss': 0.3125, 'learning_rate': 1.6952380952380955e-05, 'epoch': 2.29}\n",
      "{'loss': 0.3157, 'learning_rate': 1.688888888888889e-05, 'epoch': 2.33}\n",
      "{'loss': 0.3145, 'learning_rate': 1.6825396825396828e-05, 'epoch': 2.38}\n",
      "{'loss': 0.3118, 'learning_rate': 1.6761904761904764e-05, 'epoch': 2.43}\n",
      "{'loss': 0.3127, 'learning_rate': 1.66984126984127e-05, 'epoch': 2.48}\n",
      "{'loss': 0.3097, 'learning_rate': 1.6634920634920637e-05, 'epoch': 2.52}\n",
      "{'loss': 0.3111, 'learning_rate': 1.6571428571428574e-05, 'epoch': 2.57}\n",
      "{'loss': 0.3114, 'learning_rate': 1.6507936507936507e-05, 'epoch': 2.62}\n",
      "{'loss': 0.3109, 'learning_rate': 1.6444444444444444e-05, 'epoch': 2.67}\n",
      "{'loss': 0.3087, 'learning_rate': 1.6380952380952384e-05, 'epoch': 2.71}\n",
      "{'loss': 0.3101, 'learning_rate': 1.631746031746032e-05, 'epoch': 2.76}\n",
      "{'loss': 0.3097, 'learning_rate': 1.6253968253968257e-05, 'epoch': 2.81}\n",
      "{'loss': 0.3091, 'learning_rate': 1.6190476190476193e-05, 'epoch': 2.86}\n",
      "{'loss': 0.3115, 'learning_rate': 1.612698412698413e-05, 'epoch': 2.9}\n",
      "{'loss': 0.3093, 'learning_rate': 1.6063492063492066e-05, 'epoch': 2.95}\n",
      "{'loss': 0.3126, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 0.33103302121162415, 'eval_runtime': 4.9208, 'eval_samples_per_second': 101.61, 'eval_steps_per_second': 4.268, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3074, 'learning_rate': 1.5936507936507936e-05, 'epoch': 3.05}\n",
      "{'loss': 0.3069, 'learning_rate': 1.5873015873015872e-05, 'epoch': 3.1}\n",
      "{'loss': 0.3073, 'learning_rate': 1.580952380952381e-05, 'epoch': 3.14}\n",
      "{'loss': 0.3065, 'learning_rate': 1.5746031746031745e-05, 'epoch': 3.19}\n",
      "{'loss': 0.3079, 'learning_rate': 1.5682539682539685e-05, 'epoch': 3.24}\n",
      "{'loss': 0.3047, 'learning_rate': 1.5619047619047622e-05, 'epoch': 3.29}\n",
      "{'loss': 0.3073, 'learning_rate': 1.555555555555556e-05, 'epoch': 3.33}\n",
      "{'loss': 0.3056, 'learning_rate': 1.5492063492063495e-05, 'epoch': 3.38}\n",
      "{'loss': 0.3051, 'learning_rate': 1.542857142857143e-05, 'epoch': 3.43}\n",
      "{'loss': 0.3044, 'learning_rate': 1.5365079365079368e-05, 'epoch': 3.48}\n",
      "{'loss': 0.3075, 'learning_rate': 1.53015873015873e-05, 'epoch': 3.52}\n",
      "{'loss': 0.3062, 'learning_rate': 1.523809523809524e-05, 'epoch': 3.57}\n",
      "{'loss': 0.3054, 'learning_rate': 1.5174603174603176e-05, 'epoch': 3.62}\n",
      "{'loss': 0.3072, 'learning_rate': 1.5111111111111112e-05, 'epoch': 3.67}\n",
      "{'loss': 0.3034, 'learning_rate': 1.5047619047619049e-05, 'epoch': 3.71}\n",
      "{'loss': 0.3032, 'learning_rate': 1.4984126984126985e-05, 'epoch': 3.76}\n",
      "{'loss': 0.3032, 'learning_rate': 1.4920634920634922e-05, 'epoch': 3.81}\n",
      "{'loss': 0.3037, 'learning_rate': 1.4857142857142858e-05, 'epoch': 3.86}\n",
      "{'loss': 0.3051, 'learning_rate': 1.4793650793650795e-05, 'epoch': 3.9}\n",
      "{'loss': 0.3054, 'learning_rate': 1.4730158730158733e-05, 'epoch': 3.95}\n",
      "{'loss': 0.3028, 'learning_rate': 1.4666666666666666e-05, 'epoch': 4.0}\n",
      "{'eval_loss': 0.3272836208343506, 'eval_runtime': 4.7805, 'eval_samples_per_second': 104.591, 'eval_steps_per_second': 4.393, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3021, 'learning_rate': 1.4603174603174603e-05, 'epoch': 4.05}\n",
      "{'loss': 0.303, 'learning_rate': 1.4539682539682541e-05, 'epoch': 4.1}\n",
      "{'loss': 0.3036, 'learning_rate': 1.4476190476190478e-05, 'epoch': 4.14}\n",
      "{'loss': 0.2999, 'learning_rate': 1.4412698412698414e-05, 'epoch': 4.19}\n",
      "{'loss': 0.3015, 'learning_rate': 1.434920634920635e-05, 'epoch': 4.24}\n",
      "{'loss': 0.3027, 'learning_rate': 1.4285714285714287e-05, 'epoch': 4.29}\n",
      "{'loss': 0.3054, 'learning_rate': 1.4222222222222224e-05, 'epoch': 4.33}\n",
      "{'loss': 0.3012, 'learning_rate': 1.415873015873016e-05, 'epoch': 4.38}\n",
      "{'loss': 0.3036, 'learning_rate': 1.4095238095238097e-05, 'epoch': 4.43}\n",
      "{'loss': 0.301, 'learning_rate': 1.4031746031746032e-05, 'epoch': 4.48}\n",
      "{'loss': 0.3012, 'learning_rate': 1.3968253968253968e-05, 'epoch': 4.52}\n",
      "{'loss': 0.3013, 'learning_rate': 1.3904761904761905e-05, 'epoch': 4.57}\n",
      "{'loss': 0.2999, 'learning_rate': 1.3841269841269843e-05, 'epoch': 4.62}\n",
      "{'loss': 0.3011, 'learning_rate': 1.377777777777778e-05, 'epoch': 4.67}\n",
      "{'loss': 0.3005, 'learning_rate': 1.3714285714285716e-05, 'epoch': 4.71}\n",
      "{'loss': 0.2999, 'learning_rate': 1.3650793650793652e-05, 'epoch': 4.76}\n",
      "{'loss': 0.3022, 'learning_rate': 1.3587301587301589e-05, 'epoch': 4.81}\n",
      "{'loss': 0.3011, 'learning_rate': 1.3523809523809525e-05, 'epoch': 4.86}\n",
      "{'loss': 0.3011, 'learning_rate': 1.3460317460317462e-05, 'epoch': 4.9}\n",
      "{'loss': 0.2997, 'learning_rate': 1.3396825396825397e-05, 'epoch': 4.95}\n",
      "{'loss': 0.2985, 'learning_rate': 1.3333333333333333e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.3241373896598816, 'eval_runtime': 4.7711, 'eval_samples_per_second': 104.798, 'eval_steps_per_second': 4.402, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3017, 'learning_rate': 1.326984126984127e-05, 'epoch': 5.05}\n",
      "{'loss': 0.299, 'learning_rate': 1.3206349206349206e-05, 'epoch': 5.1}\n",
      "{'loss': 0.3003, 'learning_rate': 1.3142857142857145e-05, 'epoch': 5.14}\n",
      "{'loss': 0.2995, 'learning_rate': 1.3079365079365081e-05, 'epoch': 5.19}\n",
      "{'loss': 0.2976, 'learning_rate': 1.3015873015873018e-05, 'epoch': 5.24}\n",
      "{'loss': 0.2988, 'learning_rate': 1.2952380952380954e-05, 'epoch': 5.29}\n",
      "{'loss': 0.2982, 'learning_rate': 1.288888888888889e-05, 'epoch': 5.33}\n",
      "{'loss': 0.2957, 'learning_rate': 1.2825396825396827e-05, 'epoch': 5.38}\n",
      "{'loss': 0.299, 'learning_rate': 1.2761904761904762e-05, 'epoch': 5.43}\n",
      "{'loss': 0.3002, 'learning_rate': 1.2698412698412699e-05, 'epoch': 5.48}\n",
      "{'loss': 0.2988, 'learning_rate': 1.2634920634920635e-05, 'epoch': 5.52}\n",
      "{'loss': 0.2967, 'learning_rate': 1.2571428571428572e-05, 'epoch': 5.57}\n",
      "{'loss': 0.3002, 'learning_rate': 1.2507936507936508e-05, 'epoch': 5.62}\n",
      "{'loss': 0.298, 'learning_rate': 1.2444444444444446e-05, 'epoch': 5.67}\n",
      "{'loss': 0.2976, 'learning_rate': 1.2380952380952383e-05, 'epoch': 5.71}\n",
      "{'loss': 0.2974, 'learning_rate': 1.231746031746032e-05, 'epoch': 5.76}\n",
      "{'loss': 0.3003, 'learning_rate': 1.2253968253968256e-05, 'epoch': 5.81}\n",
      "{'loss': 0.2975, 'learning_rate': 1.2190476190476192e-05, 'epoch': 5.86}\n",
      "{'loss': 0.2976, 'learning_rate': 1.2126984126984127e-05, 'epoch': 5.9}\n",
      "{'loss': 0.2966, 'learning_rate': 1.2063492063492064e-05, 'epoch': 5.95}\n",
      "{'loss': 0.296, 'learning_rate': 1.2e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 0.32588550448417664, 'eval_runtime': 4.9134, 'eval_samples_per_second': 101.762, 'eval_steps_per_second': 4.274, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2966, 'learning_rate': 1.1936507936507937e-05, 'epoch': 6.05}\n",
      "{'loss': 0.2984, 'learning_rate': 1.1873015873015873e-05, 'epoch': 6.1}\n",
      "{'loss': 0.2975, 'learning_rate': 1.180952380952381e-05, 'epoch': 6.14}\n",
      "{'loss': 0.298, 'learning_rate': 1.1746031746031748e-05, 'epoch': 6.19}\n",
      "{'loss': 0.2982, 'learning_rate': 1.1682539682539685e-05, 'epoch': 6.24}\n",
      "{'loss': 0.2954, 'learning_rate': 1.1619047619047621e-05, 'epoch': 6.29}\n",
      "{'loss': 0.2971, 'learning_rate': 1.1555555555555556e-05, 'epoch': 6.33}\n",
      "{'loss': 0.2959, 'learning_rate': 1.1492063492063492e-05, 'epoch': 6.38}\n",
      "{'loss': 0.2962, 'learning_rate': 1.1428571428571429e-05, 'epoch': 6.43}\n",
      "{'loss': 0.2949, 'learning_rate': 1.1365079365079366e-05, 'epoch': 6.48}\n",
      "{'loss': 0.296, 'learning_rate': 1.1301587301587302e-05, 'epoch': 6.52}\n",
      "{'loss': 0.2962, 'learning_rate': 1.1238095238095239e-05, 'epoch': 6.57}\n",
      "{'loss': 0.2948, 'learning_rate': 1.1174603174603175e-05, 'epoch': 6.62}\n",
      "{'loss': 0.3008, 'learning_rate': 1.1111111111111113e-05, 'epoch': 6.67}\n",
      "{'loss': 0.2972, 'learning_rate': 1.104761904761905e-05, 'epoch': 6.71}\n",
      "{'loss': 0.2966, 'learning_rate': 1.0984126984126986e-05, 'epoch': 6.76}\n",
      "{'loss': 0.2964, 'learning_rate': 1.0920634920634921e-05, 'epoch': 6.81}\n",
      "{'loss': 0.2981, 'learning_rate': 1.0857142857142858e-05, 'epoch': 6.86}\n",
      "{'loss': 0.298, 'learning_rate': 1.0793650793650794e-05, 'epoch': 6.9}\n",
      "{'loss': 0.2974, 'learning_rate': 1.073015873015873e-05, 'epoch': 6.95}\n",
      "{'loss': 0.2967, 'learning_rate': 1.0666666666666667e-05, 'epoch': 7.0}\n",
      "{'eval_loss': 0.32007741928100586, 'eval_runtime': 4.7691, 'eval_samples_per_second': 104.842, 'eval_steps_per_second': 4.403, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2969, 'learning_rate': 1.0603174603174604e-05, 'epoch': 7.05}\n",
      "{'loss': 0.2961, 'learning_rate': 1.053968253968254e-05, 'epoch': 7.1}\n",
      "{'loss': 0.2948, 'learning_rate': 1.0476190476190477e-05, 'epoch': 7.14}\n",
      "{'loss': 0.2969, 'learning_rate': 1.0412698412698415e-05, 'epoch': 7.19}\n",
      "{'loss': 0.2953, 'learning_rate': 1.0349206349206352e-05, 'epoch': 7.24}\n",
      "{'loss': 0.2969, 'learning_rate': 1.0285714285714285e-05, 'epoch': 7.29}\n",
      "{'loss': 0.2933, 'learning_rate': 1.0222222222222223e-05, 'epoch': 7.33}\n",
      "{'loss': 0.2975, 'learning_rate': 1.015873015873016e-05, 'epoch': 7.38}\n",
      "{'loss': 0.2971, 'learning_rate': 1.0095238095238096e-05, 'epoch': 7.43}\n",
      "{'loss': 0.2964, 'learning_rate': 1.0031746031746033e-05, 'epoch': 7.48}\n",
      "{'loss': 0.295, 'learning_rate': 9.968253968253969e-06, 'epoch': 7.52}\n",
      "{'loss': 0.2956, 'learning_rate': 9.904761904761906e-06, 'epoch': 7.57}\n",
      "{'loss': 0.2957, 'learning_rate': 9.841269841269842e-06, 'epoch': 7.62}\n",
      "{'loss': 0.2936, 'learning_rate': 9.777777777777779e-06, 'epoch': 7.67}\n",
      "{'loss': 0.2964, 'learning_rate': 9.714285714285715e-06, 'epoch': 7.71}\n",
      "{'loss': 0.2948, 'learning_rate': 9.650793650793652e-06, 'epoch': 7.76}\n",
      "{'loss': 0.2952, 'learning_rate': 9.587301587301588e-06, 'epoch': 7.81}\n",
      "{'loss': 0.2942, 'learning_rate': 9.523809523809525e-06, 'epoch': 7.86}\n",
      "{'loss': 0.2947, 'learning_rate': 9.460317460317461e-06, 'epoch': 7.9}\n",
      "{'loss': 0.296, 'learning_rate': 9.396825396825398e-06, 'epoch': 7.95}\n",
      "{'loss': 0.2947, 'learning_rate': 9.333333333333334e-06, 'epoch': 8.0}\n",
      "{'eval_loss': 0.32320794463157654, 'eval_runtime': 4.9204, 'eval_samples_per_second': 101.618, 'eval_steps_per_second': 4.268, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2967, 'learning_rate': 9.26984126984127e-06, 'epoch': 8.05}\n",
      "{'loss': 0.2941, 'learning_rate': 9.206349206349207e-06, 'epoch': 8.1}\n",
      "{'loss': 0.2938, 'learning_rate': 9.142857142857144e-06, 'epoch': 8.14}\n",
      "{'loss': 0.295, 'learning_rate': 9.07936507936508e-06, 'epoch': 8.19}\n",
      "{'loss': 0.295, 'learning_rate': 9.015873015873017e-06, 'epoch': 8.24}\n",
      "{'loss': 0.2932, 'learning_rate': 8.952380952380953e-06, 'epoch': 8.29}\n",
      "{'loss': 0.2911, 'learning_rate': 8.888888888888888e-06, 'epoch': 8.33}\n",
      "{'loss': 0.293, 'learning_rate': 8.825396825396827e-06, 'epoch': 8.38}\n",
      "{'loss': 0.2948, 'learning_rate': 8.761904761904763e-06, 'epoch': 8.43}\n",
      "{'loss': 0.2966, 'learning_rate': 8.6984126984127e-06, 'epoch': 8.48}\n",
      "{'loss': 0.2939, 'learning_rate': 8.634920634920636e-06, 'epoch': 8.52}\n",
      "{'loss': 0.2926, 'learning_rate': 8.571428571428571e-06, 'epoch': 8.57}\n",
      "{'loss': 0.2934, 'learning_rate': 8.507936507936509e-06, 'epoch': 8.62}\n",
      "{'loss': 0.2942, 'learning_rate': 8.444444444444446e-06, 'epoch': 8.67}\n",
      "{'loss': 0.2955, 'learning_rate': 8.380952380952382e-06, 'epoch': 8.71}\n",
      "{'loss': 0.2945, 'learning_rate': 8.317460317460319e-06, 'epoch': 8.76}\n",
      "{'loss': 0.2934, 'learning_rate': 8.253968253968254e-06, 'epoch': 8.81}\n",
      "{'loss': 0.2941, 'learning_rate': 8.190476190476192e-06, 'epoch': 8.86}\n",
      "{'loss': 0.2956, 'learning_rate': 8.126984126984128e-06, 'epoch': 8.9}\n",
      "{'loss': 0.2937, 'learning_rate': 8.063492063492065e-06, 'epoch': 8.95}\n",
      "{'loss': 0.2948, 'learning_rate': 8.000000000000001e-06, 'epoch': 9.0}\n",
      "{'eval_loss': 0.32645946741104126, 'eval_runtime': 4.7787, 'eval_samples_per_second': 104.631, 'eval_steps_per_second': 4.394, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2931, 'learning_rate': 7.936507936507936e-06, 'epoch': 9.05}\n",
      "{'loss': 0.2945, 'learning_rate': 7.873015873015873e-06, 'epoch': 9.1}\n",
      "{'loss': 0.294, 'learning_rate': 7.809523809523811e-06, 'epoch': 9.14}\n",
      "{'loss': 0.2938, 'learning_rate': 7.746031746031747e-06, 'epoch': 9.19}\n",
      "{'loss': 0.2929, 'learning_rate': 7.682539682539684e-06, 'epoch': 9.24}\n",
      "{'loss': 0.2928, 'learning_rate': 7.61904761904762e-06, 'epoch': 9.29}\n",
      "{'loss': 0.2926, 'learning_rate': 7.555555555555556e-06, 'epoch': 9.33}\n",
      "{'loss': 0.2935, 'learning_rate': 7.492063492063493e-06, 'epoch': 9.38}\n",
      "{'loss': 0.2926, 'learning_rate': 7.428571428571429e-06, 'epoch': 9.43}\n",
      "{'loss': 0.2935, 'learning_rate': 7.3650793650793666e-06, 'epoch': 9.48}\n",
      "{'loss': 0.2946, 'learning_rate': 7.301587301587301e-06, 'epoch': 9.52}\n",
      "{'loss': 0.2934, 'learning_rate': 7.238095238095239e-06, 'epoch': 9.57}\n",
      "{'loss': 0.2934, 'learning_rate': 7.174603174603175e-06, 'epoch': 9.62}\n",
      "{'loss': 0.294, 'learning_rate': 7.111111111111112e-06, 'epoch': 9.67}\n",
      "{'loss': 0.2934, 'learning_rate': 7.047619047619048e-06, 'epoch': 9.71}\n",
      "{'loss': 0.2926, 'learning_rate': 6.984126984126984e-06, 'epoch': 9.76}\n",
      "{'loss': 0.2927, 'learning_rate': 6.920634920634921e-06, 'epoch': 9.81}\n",
      "{'loss': 0.2927, 'learning_rate': 6.857142857142858e-06, 'epoch': 9.86}\n",
      "{'loss': 0.2929, 'learning_rate': 6.7936507936507944e-06, 'epoch': 9.9}\n",
      "{'loss': 0.2935, 'learning_rate': 6.730158730158731e-06, 'epoch': 9.95}\n",
      "{'loss': 0.2916, 'learning_rate': 6.666666666666667e-06, 'epoch': 10.0}\n",
      "{'eval_loss': 0.32380008697509766, 'eval_runtime': 4.7756, 'eval_samples_per_second': 104.698, 'eval_steps_per_second': 4.397, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2937, 'learning_rate': 6.603174603174603e-06, 'epoch': 10.05}\n",
      "{'loss': 0.2922, 'learning_rate': 6.5396825396825405e-06, 'epoch': 10.1}\n",
      "{'loss': 0.2921, 'learning_rate': 6.476190476190477e-06, 'epoch': 10.14}\n",
      "{'loss': 0.2912, 'learning_rate': 6.412698412698414e-06, 'epoch': 10.19}\n",
      "{'loss': 0.2938, 'learning_rate': 6.349206349206349e-06, 'epoch': 10.24}\n",
      "{'loss': 0.293, 'learning_rate': 6.285714285714286e-06, 'epoch': 10.29}\n",
      "{'loss': 0.2925, 'learning_rate': 6.222222222222223e-06, 'epoch': 10.33}\n",
      "{'loss': 0.2924, 'learning_rate': 6.15873015873016e-06, 'epoch': 10.38}\n",
      "{'loss': 0.2913, 'learning_rate': 6.095238095238096e-06, 'epoch': 10.43}\n",
      "{'loss': 0.2933, 'learning_rate': 6.031746031746032e-06, 'epoch': 10.48}\n",
      "{'loss': 0.2929, 'learning_rate': 5.968253968253968e-06, 'epoch': 10.52}\n",
      "{'loss': 0.2945, 'learning_rate': 5.904761904761905e-06, 'epoch': 10.57}\n",
      "{'loss': 0.2934, 'learning_rate': 5.841269841269842e-06, 'epoch': 10.62}\n",
      "{'loss': 0.29, 'learning_rate': 5.777777777777778e-06, 'epoch': 10.67}\n",
      "{'loss': 0.2935, 'learning_rate': 5.7142857142857145e-06, 'epoch': 10.71}\n",
      "{'loss': 0.2913, 'learning_rate': 5.650793650793651e-06, 'epoch': 10.76}\n",
      "{'loss': 0.2908, 'learning_rate': 5.5873015873015876e-06, 'epoch': 10.81}\n",
      "{'loss': 0.2936, 'learning_rate': 5.523809523809525e-06, 'epoch': 10.86}\n",
      "{'loss': 0.2922, 'learning_rate': 5.460317460317461e-06, 'epoch': 10.9}\n",
      "{'loss': 0.2931, 'learning_rate': 5.396825396825397e-06, 'epoch': 10.95}\n",
      "{'loss': 0.2951, 'learning_rate': 5.333333333333334e-06, 'epoch': 11.0}\n",
      "{'eval_loss': 0.32237866520881653, 'eval_runtime': 4.7732, 'eval_samples_per_second': 104.752, 'eval_steps_per_second': 4.4, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2903, 'learning_rate': 5.26984126984127e-06, 'epoch': 11.05}\n",
      "{'loss': 0.2908, 'learning_rate': 5.2063492063492076e-06, 'epoch': 11.1}\n",
      "{'loss': 0.2926, 'learning_rate': 5.142857142857142e-06, 'epoch': 11.14}\n",
      "{'loss': 0.2924, 'learning_rate': 5.07936507936508e-06, 'epoch': 11.19}\n",
      "{'loss': 0.2926, 'learning_rate': 5.015873015873016e-06, 'epoch': 11.24}\n",
      "{'loss': 0.2922, 'learning_rate': 4.952380952380953e-06, 'epoch': 11.29}\n",
      "{'loss': 0.2918, 'learning_rate': 4.888888888888889e-06, 'epoch': 11.33}\n",
      "{'loss': 0.2926, 'learning_rate': 4.825396825396826e-06, 'epoch': 11.38}\n",
      "{'loss': 0.2928, 'learning_rate': 4.761904761904762e-06, 'epoch': 11.43}\n",
      "{'loss': 0.2904, 'learning_rate': 4.698412698412699e-06, 'epoch': 11.48}\n",
      "{'loss': 0.2909, 'learning_rate': 4.634920634920635e-06, 'epoch': 11.52}\n",
      "{'loss': 0.291, 'learning_rate': 4.571428571428572e-06, 'epoch': 11.57}\n",
      "{'loss': 0.2921, 'learning_rate': 4.5079365079365085e-06, 'epoch': 11.62}\n",
      "{'loss': 0.2899, 'learning_rate': 4.444444444444444e-06, 'epoch': 11.67}\n",
      "{'loss': 0.292, 'learning_rate': 4.3809523809523815e-06, 'epoch': 11.71}\n",
      "{'loss': 0.2907, 'learning_rate': 4.317460317460318e-06, 'epoch': 11.76}\n",
      "{'loss': 0.2916, 'learning_rate': 4.2539682539682546e-06, 'epoch': 11.81}\n",
      "{'loss': 0.2928, 'learning_rate': 4.190476190476191e-06, 'epoch': 11.86}\n",
      "{'loss': 0.2913, 'learning_rate': 4.126984126984127e-06, 'epoch': 11.9}\n",
      "{'loss': 0.2915, 'learning_rate': 4.063492063492064e-06, 'epoch': 11.95}\n",
      "{'loss': 0.2907, 'learning_rate': 4.000000000000001e-06, 'epoch': 12.0}\n",
      "{'eval_loss': 0.311610609292984, 'eval_runtime': 4.9246, 'eval_samples_per_second': 101.532, 'eval_steps_per_second': 4.264, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2919, 'learning_rate': 3.936507936507936e-06, 'epoch': 12.05}\n",
      "{'loss': 0.2897, 'learning_rate': 3.873015873015874e-06, 'epoch': 12.1}\n",
      "{'loss': 0.2909, 'learning_rate': 3.80952380952381e-06, 'epoch': 12.14}\n",
      "{'loss': 0.2915, 'learning_rate': 3.7460317460317463e-06, 'epoch': 12.19}\n",
      "{'loss': 0.2915, 'learning_rate': 3.6825396825396833e-06, 'epoch': 12.24}\n",
      "{'loss': 0.2911, 'learning_rate': 3.6190476190476194e-06, 'epoch': 12.29}\n",
      "{'loss': 0.2912, 'learning_rate': 3.555555555555556e-06, 'epoch': 12.33}\n",
      "{'loss': 0.2918, 'learning_rate': 3.492063492063492e-06, 'epoch': 12.38}\n",
      "{'loss': 0.2912, 'learning_rate': 3.428571428571429e-06, 'epoch': 12.43}\n",
      "{'loss': 0.2913, 'learning_rate': 3.3650793650793655e-06, 'epoch': 12.48}\n",
      "{'loss': 0.291, 'learning_rate': 3.3015873015873016e-06, 'epoch': 12.52}\n",
      "{'loss': 0.2902, 'learning_rate': 3.2380952380952385e-06, 'epoch': 12.57}\n",
      "{'loss': 0.2909, 'learning_rate': 3.1746031746031746e-06, 'epoch': 12.62}\n",
      "{'loss': 0.2904, 'learning_rate': 3.1111111111111116e-06, 'epoch': 12.67}\n",
      "{'loss': 0.2929, 'learning_rate': 3.047619047619048e-06, 'epoch': 12.71}\n",
      "{'loss': 0.2908, 'learning_rate': 2.984126984126984e-06, 'epoch': 12.76}\n",
      "{'loss': 0.2921, 'learning_rate': 2.920634920634921e-06, 'epoch': 12.81}\n",
      "{'loss': 0.2921, 'learning_rate': 2.8571428571428573e-06, 'epoch': 12.86}\n",
      "{'loss': 0.2908, 'learning_rate': 2.7936507936507938e-06, 'epoch': 12.9}\n",
      "{'loss': 0.2899, 'learning_rate': 2.7301587301587303e-06, 'epoch': 12.95}\n",
      "{'loss': 0.2913, 'learning_rate': 2.666666666666667e-06, 'epoch': 13.0}\n",
      "{'eval_loss': 0.32192644476890564, 'eval_runtime': 4.768, 'eval_samples_per_second': 104.865, 'eval_steps_per_second': 4.404, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2922, 'learning_rate': 2.6031746031746038e-06, 'epoch': 13.05}\n",
      "{'loss': 0.2913, 'learning_rate': 2.53968253968254e-06, 'epoch': 13.1}\n",
      "{'loss': 0.2921, 'learning_rate': 2.4761904761904764e-06, 'epoch': 13.14}\n",
      "{'loss': 0.2907, 'learning_rate': 2.412698412698413e-06, 'epoch': 13.19}\n",
      "{'loss': 0.292, 'learning_rate': 2.3492063492063494e-06, 'epoch': 13.24}\n",
      "{'loss': 0.2904, 'learning_rate': 2.285714285714286e-06, 'epoch': 13.29}\n",
      "{'loss': 0.2903, 'learning_rate': 2.222222222222222e-06, 'epoch': 13.33}\n",
      "{'loss': 0.2909, 'learning_rate': 2.158730158730159e-06, 'epoch': 13.38}\n",
      "{'loss': 0.2916, 'learning_rate': 2.0952380952380955e-06, 'epoch': 13.43}\n",
      "{'loss': 0.2905, 'learning_rate': 2.031746031746032e-06, 'epoch': 13.48}\n",
      "{'loss': 0.2909, 'learning_rate': 1.968253968253968e-06, 'epoch': 13.52}\n",
      "{'loss': 0.2916, 'learning_rate': 1.904761904761905e-06, 'epoch': 13.57}\n",
      "{'loss': 0.2897, 'learning_rate': 1.8412698412698416e-06, 'epoch': 13.62}\n",
      "{'loss': 0.2901, 'learning_rate': 1.777777777777778e-06, 'epoch': 13.67}\n",
      "{'loss': 0.2911, 'learning_rate': 1.7142857142857145e-06, 'epoch': 13.71}\n",
      "{'loss': 0.2899, 'learning_rate': 1.6507936507936508e-06, 'epoch': 13.76}\n",
      "{'loss': 0.289, 'learning_rate': 1.5873015873015873e-06, 'epoch': 13.81}\n",
      "{'loss': 0.2895, 'learning_rate': 1.523809523809524e-06, 'epoch': 13.86}\n",
      "{'loss': 0.2911, 'learning_rate': 1.4603174603174606e-06, 'epoch': 13.9}\n",
      "{'loss': 0.2922, 'learning_rate': 1.3968253968253969e-06, 'epoch': 13.95}\n",
      "{'loss': 0.2918, 'learning_rate': 1.3333333333333334e-06, 'epoch': 14.0}\n",
      "{'eval_loss': 0.31965863704681396, 'eval_runtime': 4.7775, 'eval_samples_per_second': 104.658, 'eval_steps_per_second': 4.396, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2898, 'learning_rate': 1.26984126984127e-06, 'epoch': 14.05}\n",
      "{'loss': 0.2911, 'learning_rate': 1.2063492063492065e-06, 'epoch': 14.1}\n",
      "{'loss': 0.2921, 'learning_rate': 1.142857142857143e-06, 'epoch': 14.14}\n",
      "{'loss': 0.2897, 'learning_rate': 1.0793650793650795e-06, 'epoch': 14.19}\n",
      "{'loss': 0.2923, 'learning_rate': 1.015873015873016e-06, 'epoch': 14.24}\n",
      "{'loss': 0.2919, 'learning_rate': 9.523809523809525e-07, 'epoch': 14.29}\n",
      "{'loss': 0.2902, 'learning_rate': 8.88888888888889e-07, 'epoch': 14.33}\n",
      "{'loss': 0.2886, 'learning_rate': 8.253968253968254e-07, 'epoch': 14.38}\n",
      "{'loss': 0.2885, 'learning_rate': 7.61904761904762e-07, 'epoch': 14.43}\n",
      "{'loss': 0.2892, 'learning_rate': 6.984126984126984e-07, 'epoch': 14.48}\n",
      "{'loss': 0.2924, 'learning_rate': 6.34920634920635e-07, 'epoch': 14.52}\n",
      "{'loss': 0.2913, 'learning_rate': 5.714285714285715e-07, 'epoch': 14.57}\n",
      "{'loss': 0.2905, 'learning_rate': 5.07936507936508e-07, 'epoch': 14.62}\n",
      "{'loss': 0.2908, 'learning_rate': 4.444444444444445e-07, 'epoch': 14.67}\n",
      "{'loss': 0.2916, 'learning_rate': 3.80952380952381e-07, 'epoch': 14.71}\n",
      "{'loss': 0.2909, 'learning_rate': 3.174603174603175e-07, 'epoch': 14.76}\n",
      "{'loss': 0.2903, 'learning_rate': 2.53968253968254e-07, 'epoch': 14.81}\n",
      "{'loss': 0.2898, 'learning_rate': 1.904761904761905e-07, 'epoch': 14.86}\n",
      "{'loss': 0.291, 'learning_rate': 1.26984126984127e-07, 'epoch': 14.9}\n",
      "{'loss': 0.2893, 'learning_rate': 6.34920634920635e-08, 'epoch': 14.95}\n",
      "{'loss': 0.2909, 'learning_rate': 0.0, 'epoch': 15.0}\n",
      "{'eval_loss': 0.31911298632621765, 'eval_runtime': 4.7738, 'eval_samples_per_second': 104.738, 'eval_steps_per_second': 4.399, 'epoch': 15.0}\n",
      "{'train_runtime': 730.1889, 'train_samples_per_second': 51.357, 'train_steps_per_second': 2.157, 'train_loss': 0.30666147890545076, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1575, training_loss=0.30666147890545076, metrics={'train_runtime': 730.1889, 'train_samples_per_second': 51.357, 'train_steps_per_second': 2.157, 'train_loss': 0.30666147890545076, 'epoch': 15.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▆▅█▇▆▆▄▅▆▅▅▁▅▄▄</td></tr><tr><td>eval/runtime</td><td>▁▁█▂▁▇▁█▁▁▁█▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>██▁▇█▁█▁███▁███</td></tr><tr><td>eval/steps_per_second</td><td>██▁▇█▂█▁███▁███</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.31911</td></tr><tr><td>eval/runtime</td><td>4.7738</td></tr><tr><td>eval/samples_per_second</td><td>104.738</td></tr><tr><td>eval/steps_per_second</td><td>4.399</td></tr><tr><td>train/epoch</td><td>15.0</td></tr><tr><td>train/global_step</td><td>1575</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2909</td></tr><tr><td>train/total_flos</td><td>7830699692544000.0</td></tr><tr><td>train/train_loss</td><td>0.30666</td></tr><tr><td>train/train_runtime</td><td>730.1889</td></tr><tr><td>train/train_samples_per_second</td><td>51.357</td></tr><tr><td>train/train_steps_per_second</td><td>2.157</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt2-append-autoreg-str-tokenizer_default-vars_10-rules_10-train_2500-test_500</strong> at: <a href='https://wandb.ai/transformer_friends/transformer_friends/runs/arqgxoex' target=\"_blank\">https://wandb.ai/transformer_friends/transformer_friends/runs/arqgxoex</a><br/>Synced 6 W&B file(s), 0 media file(s), 9 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231214_184038-arqgxoex/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makhare\u001b[0m (\u001b[33mtransformer_friends\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/akhare/repos/tf_logic/notebooks/wandb/run-20231214_191935-iotxkobe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/transformer_friends/transformer_friends/runs/iotxkobe' target=\"_blank\">gpt2-append-autoreg-str-tokenizer_default-vars_10-rules_10-train_2500-test_500</a></strong> to <a href='https://wandb.ai/transformer_friends/transformer_friends' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/transformer_friends/transformer_friends' target=\"_blank\">https://wandb.ai/transformer_friends/transformer_friends</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/transformer_friends/transformer_friends/runs/iotxkobe' target=\"_blank\">https://wandb.ai/transformer_friends/transformer_friends/runs/iotxkobe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt2_append_autoreg_str_results/checkpoint-1260\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/trainer.py:2748\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2747\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2748\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2749\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:81\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     79\u001b[0m         thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     80\u001b[0m     \u001b[39mfor\u001b[39;00m thread \u001b[39min\u001b[39;00m threads:\n\u001b[0;32m---> 81\u001b[0m         thread\u001b[39m.\u001b[39mjoin()\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     _worker(\u001b[39m0\u001b[39m, modules[\u001b[39m0\u001b[39m], inputs[\u001b[39m0\u001b[39m], kwargs_tup[\u001b[39m0\u001b[39m], devices[\u001b[39m0\u001b[39m], streams[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=\"gpt2_append_autoreg_str_results/checkpoint-1260\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[RULES_START] [RULE_START] x0 , x6 -> x0 , x1 , x2 [RULE_END] [RULE_START] empty -> x0 , x2 [RULE_END] [RULE_START] x3 , x7 , x8 -> x2 , x4 , x5 , x7 , x8 [RULE_END] [RULE_START] x1 , x3 , x9 -> empty [RULE_END] [RULE_START] x1 , x2 , x3 , x5 -> x1 , x2 , x5 [RULE_END] [RULE_START] x3 , x4 , x5 , x6 , x7 -> x1 , x7 [RULE_END] [RULE_START] x1 , x4 , x7 -> empty [RULE_END] [RULE_START] empty -> x2 , x5 , x8 [RULE_END] [RULE_START] x3 , x4 , x8 -> x0 , x6 [RULE_END] [RULE_START] x0 , x2 -> x6 [RULE_END] [RULES_END] [STATES_START] [STATE_START] x3 , x8 [STATE_END] [STATE_START]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m test_data_new \u001b[39m=\u001b[39m test_data[\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "test_data_new = test_data[0][\"input_ids\"].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = tokenizer(test_data[1][\"prompt\"], return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "model_cpu = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(test_data[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         input_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         max_new_tokens\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2267\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2268\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2269\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m         )\n\u001b[0;32m-> 2271\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "input_ids = tokenizer(test_data[0][\"prompt\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "output = model.to(\"cpu\").generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "output = output[0].to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(target, pred):\n",
    "    \"\"\"\n",
    "    Check if the final state of the target matches the final state of the prediction\n",
    "    \"\"\"\n",
    "\n",
    "    state_start = \"[STATE_START]\"\n",
    "    state_end = \"[STATE_END]\"\n",
    "    final_target_state = target.split(state_start)[-1].split(state_end)[0].split(\",\")\n",
    "    final_pred_state = pred.split(state_start)[-1].split(state_end)[0].split(\",\")\n",
    "    final_target_state_set = set(state.strip() for state in final_target_state)\n",
    "    final_pred_state_set = set(state.strip() for state in final_pred_state)\n",
    "    print(final_target_state_set)\n",
    "    print(final_pred_state_set)\n",
    "\n",
    "    return final_target_state_set == final_pred_state_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' x0 , x2 , x3 , x5 , x8 [STATE_END] [STATE_START] x0 , x2 , x3 , x5 , x6 , x8 [STATE_END] [STATE_START] x0 , x1 , x2 , x3 , x5 , x6 , x8 [STATE_END] [STATES_END]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x3', 'x6', 'x8', 'x0', 'x5', 'x1', 'x2'}\n",
      "{'x3', 'x6', 'x8', 'x0', 'x5', 'x2'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_similarity(test_data[0][\"target\"], tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(test_data)):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenizer(test_data[i][\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m             input_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     output \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     pred_ids\u001b[39m.\u001b[39mappend(output[input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:])\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/generation/utils.py:1673\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1657\u001b[0m         input_ids,\n\u001b[1;32m   1658\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1670\u001b[0m     )\n\u001b[1;32m   1671\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1672\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1674\u001b[0m         input_ids,\n\u001b[1;32m   1675\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1676\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1677\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1678\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1679\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1680\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1681\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1682\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1683\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1684\u001b[0m     )\n\u001b[1;32m   1686\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1687\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/generation/utils.py:2521\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2520\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2521\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2522\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2523\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2524\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2525\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2526\u001b[0m )\n\u001b[1;32m   2528\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2529\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1075\u001b[0m     input_ids,\n\u001b[1;32m   1076\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1077\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1078\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1079\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1080\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1081\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1082\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1083\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1084\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1085\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1086\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1087\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    889\u001b[0m         hidden_states,\n\u001b[1;32m    890\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    891\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    892\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    893\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    894\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    895\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    896\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    391\u001b[0m     hidden_states,\n\u001b[1;32m    392\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    393\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    394\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    395\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    396\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    397\u001b[0m )\n\u001b[1;32m    398\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:312\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    311\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     query, key, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_attn(hidden_states)\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_size, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    314\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    315\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/pytorch_utils.py:107\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    106\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m--> 107\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    108\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_correct = 0\n",
    "pred_ids = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    input_ids = tokenizer(test_data[i][\"prompt\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "    output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=200,\n",
    "        )\n",
    "    output = output[0].to(\"cpu\")\n",
    "    pred_ids.append(output[input_ids.shape[1]:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x5', 'x6', 'x2', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x4', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x1', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x3', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x1', 'x0'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x3'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x3', 'x8'}\n",
      "{'x5', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x2', 'x1', 'x3', 'x8'}\n",
      "{'x5', 'x2', 'x1', 'x3', 'x8'}\n",
      "{'x2', 'x7', 'x4', 'x1', 'x3', 'x0'}\n",
      "{'x7', 'x4', 'x1', 'x3', 'x0'}\n",
      "{'x5', 'x6', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x4', 'x1', 'x3', 'x'}\n",
      "{'x0', 'x1', 'x9', 'x7'}\n",
      "{'x0', 'x1', 'x9', 'x7'}\n",
      "{'x9', 'x5', 'x1', 'x2'}\n",
      "{'x9', 'x2'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x1', 'x3', 'x9', 'x0'}\n",
      "{'x5', 'x4', 'x1', 'x3', 'x9', 'x0'}\n",
      "{'x4', 'x0', 'x1', 'x3'}\n",
      "{'x6', 'x4', 'x1', 'x3', 'x0'}\n",
      "{'x5', 'x2', 'x7', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x7', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x3', 'x9', 'x0'}\n",
      "{'x5', 'x2', 'x3', 'x9', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x3', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x6', 'x7', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x1', 'x3'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x3', 'x'}\n",
      "{'x7', 'x4', 'x1', 'x9', 'x8'}\n",
      "{'x7', 'x4', 'x1', 'x9', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x1', 'x3', 'x8'}\n",
      "{'x4', 'x5', 'x6', 'x1'}\n",
      "{'x6', 'x9', 'x3', 'x8'}\n",
      "{'x6', 'x9', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x1', 'x3', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x1', 'x8'}\n",
      "{'x2', 'x4', 'x1', 'x9', 'x8'}\n",
      "{'x4', 'x1', 'x2', 'x8'}\n",
      "{'x6', 'x2', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x4', 'x6', 'x9', 'x3'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x9'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x9'}\n",
      "{'x5', 'x6', 'x7', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x1', 'x9', 'x0'}\n",
      "{'x6', 'x2', 'x4', 'x3', 'x0'}\n",
      "{'x6', 'x2', 'x4', 'x3', 'x0', 'x8'}\n",
      "{'x6', 'x4', 'x3', 'x9', 'x0'}\n",
      "{'x6', 'x4', 'x3', 'x9', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x1', 'x3'}\n",
      "{'x0', 'x6', 'x2', 'x3'}\n",
      "{'x7', 'x4', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x7', 'x4', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x1', 'x3'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x1', 'x3'}\n",
      "{'x5', 'x2', 'x4', 'x3', 'x0'}\n",
      "{'x2', 'x4', 'x3', 'x9', 'x0'}\n",
      "{'x0', 'x6', 'x9'}\n",
      "{'x0', 'x6', 'x9'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x8'}\n",
      "{'x5', 'x2', 'x8'}\n",
      "{'x6', 'x2', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x4', 'x0', 'x9', 'x5'}\n",
      "{'x5', 'x9', 'x2'}\n",
      "{'x6', 'x7', 'x1', 'x3', 'x8'}\n",
      "{'x6', 'x1', 'x3', 'x7'}\n",
      "{'x5', 'x6', 'x2'}\n",
      "{'x5', 'x6', 'x9'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x1', 'x0'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x1', 'x0'}\n",
      "{'x0', 'x6', 'x2', 'x8'}\n",
      "{'x0', 'x6', 'x2', 'x8'}\n",
      "{'x5', 'x4', 'x1', 'x3', 'x0'}\n",
      "{'x4', 'x0', 'x5'}\n",
      "{'x8', 'x6', 'x3', 'x7'}\n",
      "{'x8', 'x6', 'x3', 'x7'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x4', 'x0', 'x1', 'x2'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x4', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x4', 'x5', 'x6', 'x9'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x9'}\n",
      "{'x6', 'x7', 'x4', 'x1', 'x3'}\n",
      "{'x6', 'x7', 'x4', 'x1', 'x3'}\n",
      "{'x5', 'x7', 'x4', 'x9', 'x0'}\n",
      "{'x5', 'x7', 'x4', 'x3', 'x0', 'x'}\n",
      "{'x7', 'x4', 'x1', 'x0', 'x8'}\n",
      "{'x4', 'x0', 'x1', 'x8'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x1', 'x3', 'x0'}\n",
      "{'x2', 'x7', 'x4', 'x1', 'x3', 'x0'}\n",
      "{'x5', 'x6', 'x7', 'x1', 'x9', 'x0'}\n",
      "{'', 'x0', 'x1'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x3', 'x0'}\n",
      "{'x2', 'x7', 'x4', 'x3', 'x0'}\n",
      "{'x5', 'x6', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x9', 'x0', 'x8'}\n",
      "{'x0', 'x9', 'x2', 'x8'}\n",
      "{'x0', 'x1', 'x2', 'x3'}\n",
      "{'x0', 'x1', 'x2', 'x3'}\n",
      "{'x5', 'x6', 'x4', 'x9', 'x0', 'x8'}\n",
      "{'x4', 'x5', 'x9', 'x8'}\n",
      "{'x2', 'x7', 'x1', 'x3', 'x9', 'x0'}\n",
      "{'x2', 'x7', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x6', 'x1', 'x0', 'x8'}\n",
      "{'x0', 'x6', 'x1', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x1', 'x3'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x1', 'x3'}\n",
      "{'x6', 'x7', 'x1', 'x0', 'x8'}\n",
      "{'x6', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x0', 'x9', 'x5', 'x8'}\n",
      "{'x5', 'x9', 'x3'}\n",
      "{'x5', 'x9', 'x3'}\n",
      "{'x4', 'x6', 'x2', 'x8'}\n",
      "{'x4', 'x9', 'x2', 'x8'}\n",
      "{'x5', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x4', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x6', 'x2', 'x4', 'x1', 'x8'}\n",
      "{'x2', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x2', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x8', 'x6', 'x1', 'x7'}\n",
      "{'x8', 'x6', 'x1', 'x7'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x1', 'x9', 'x0'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x9', 'x0'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x1', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x1', 'x0'}\n",
      "{'x2', 'x4', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x4', 'x9', 'x3', 'x8'}\n",
      "{'x4', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x2', 'x4', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x1', 'x3', 'x8'}\n",
      "{'x6', 'x3', 'x8'}\n",
      "{'x6', 'x2', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x6', 'x2', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x6', 'x9', 'x3'}\n",
      "{'x5', 'x6', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x1', 'x8'}\n",
      "{'x2', 'x7', 'x4', 'x1', 'x8'}\n",
      "{'x4', 'x5', 'x1', 'x3'}\n",
      "{'x4', 'x1', 'x3'}\n",
      "{'x6', 'x4', 'x3', 'x9', 'x0'}\n",
      "{'x0', 'x6', 'x9', 'x3'}\n",
      "{'x6', 'x7', 'x3', 'x9', 'x8'}\n",
      "{'x6', 'x7', 'x3', 'x9', 'x8'}\n",
      "{'x6', 'x2', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x4', 'x9', 'x2', 'x8'}\n",
      "{'x4', 'x9', 'x2', 'x8'}\n",
      "{'x2', 'x7', 'x4', 'x3', 'x0', 'x8'}\n",
      "{'x2', 'x7', 'x4', 'x3', 'x0'}\n",
      "{'x4', 'x1', 'x3', 'x7'}\n",
      "{'x4', 'x1', 'x7'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x3'}\n",
      "{'x5', 'x1', 'x2', 'x3'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x1', 'x0'}\n",
      "{'x6', 'x2', 'x7', 'x1', 'x0'}\n",
      "{'x5', 'x7', 'x4', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x4', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x1', 'x3'}\n",
      "{'x5', 'x6', 'x2', 'x1', 'x3'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x1', 'x0'}\n",
      "{'x6', 'x7', 'x4', 'x1', 'x0'}\n",
      "{'x2', 'x7', 'x4', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x2', 'x7', 'x4', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x0', 'x8'}\n",
      "{'x5', 'x7', 'x4', 'x3', 'x8'}\n",
      "{'x4', 'x5', 'x3', 'x8'}\n",
      "{'x4', 'x5', 'x3', 'x8'}\n",
      "{'x6', 'x1', 'x2', 'x7'}\n",
      "{'x6', 'x1', 'x2', 'x7'}\n",
      "{'x5', 'x6', 'x2', 'x1', 'x3', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x1', 'x3', 'x0'}\n",
      "{'x5', 'x6', 'x7', 'x9', 'x0', 'x8'}\n",
      "{'', 'x5', 'x6', 'x7', 'x0', 'x8'}\n",
      "{'x0', 'x8', 'x9', 'x7'}\n",
      "{'x0', 'x8', 'x9', 'x7'}\n",
      "{'x5', 'x2', 'x7', 'x0', 'x8'}\n",
      "{'x0', 'x2', 'x5', 'x8'}\n",
      "{'x6', 'x4', 'x3', 'x9', 'x8'}\n",
      "{'x4', 'x6', 'x9', 'x8'}\n",
      "{'x6', 'x2', 'x4', 'x1', 'x0'}\n",
      "{'x0', 'x6', 'x1', 'x2'}\n",
      "{'x6', 'x4', 'x1', 'x3', 'x9', 'x0'}\n",
      "{'x6', 'x4', 'x1', 'x3', 'x9', 'x0'}\n",
      "{'x9', 'x1', 'x2', 'x3'}\n",
      "{'x2', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x1', 'x0', 'x8'}\n",
      "{'x6', 'x7', 'x4', 'x1', 'x0', 'x8'}\n",
      "{'x1', 'x2', 'x3', 'x8'}\n",
      "{'x1', 'x2', 'x3', 'x8'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x6', 'x4', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x6', 'x4', 'x3', 'x9', 'x8'}\n",
      "{'x7', 'x4', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x7', 'x4', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x1', 'x9', 'x8'}\n",
      "{'x1'}\n",
      "{'x5', 'x1', 'x3', 'x8'}\n",
      "{'x5', 'x1', 'x3', 'x8'}\n",
      "{'x2', 'x7', 'x1', 'x9', 'x0'}\n",
      "{'x9', 'x1', 'x2', 'x7'}\n",
      "{'x5', 'x6', 'x1', 'x9', 'x8'}\n",
      "{'x5', 'x6', 'x1', 'x9', 'x8'}\n",
      "{'x5', 'x1', 'x2', 'x7'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x1'}\n",
      "{'x9', 'x0', 'x1', 'x5'}\n",
      "{'x9', 'x0', 'x1', 'x5'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x7', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x7', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x6', 'x1', 'x9', 'x3'}\n",
      "{'x6', 'x2', 'x4', 'x3', 'x8'}\n",
      "{'x4', 'x6', 'x2', 'x3'}\n",
      "{'x4', 'x6', 'x9', 'x2'}\n",
      "{'x4', 'x6', 'x9', 'x2'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x4', 'x9', 'x0', 'x8'}\n",
      "{'x2', 'x7', 'x4', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x2', 'x7', 'x4', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x7', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x7', 'x1', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x7', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x0'}\n",
      "{'x4', 'x0', 'x6', 'x9'}\n",
      "{'x0', 'x6', 'x9'}\n",
      "{'x6', 'x2', 'x7', 'x1', 'x9'}\n",
      "{'x6', 'x2', 'x7', 'x1', 'x9'}\n",
      "{'x4', 'x6', 'x1', 'x3'}\n",
      "{'x4', 'x6', 'x1'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x9', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x9', 'x0'}\n",
      "{'x5', 'x7', 'x4', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x7', 'x4', 'x1', 'x9', 'x8'}\n",
      "{'x5', 'x7', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x7', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x4', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x4', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x9', 'x0'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x9', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'', 'x6', 'x2', 'x4', 'x3'}\n",
      "{'x6', 'x1', 'x3', 'x8'}\n",
      "{'x6', 'x1', 'x3', 'x8'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x0', 'x8'}\n",
      "{'x0', 'x', 'x1', 'x2'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x1', 'x3', 'x9', 'x0'}\n",
      "{'x1', 'x9', 'x3', 'x7'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x4', 'x3', 'x0', 'x8'}\n",
      "{'x6', 'x7', 'x4', 'x1', 'x0'}\n",
      "{'x4', 'x0', 'x1', 'x8'}\n",
      "{'x5', 'x6', 'x1', 'x2'}\n",
      "{'x5', 'x1', 'x2'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x3', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x3', 'x8'}\n",
      "{'x7', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x7', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x1', 'x7'}\n",
      "{'x5', 'x8', 'x7'}\n",
      "{'x5', 'x6', 'x4', 'x1', 'x3', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x1', 'x8'}\n",
      "{'x6', 'x2', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x2', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x1', 'x3', 'x9'}\n",
      "{'x0', 'x1', 'x2', 'x5'}\n",
      "{'x0', 'x1', 'x2', 'x5'}\n",
      "{'x5', 'x6', 'x2', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x3', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x3', 'x8'}\n",
      "{'x5', 'x7', 'x4', 'x1', 'x8'}\n",
      "{'x4', 'x5', 'x8', 'x7'}\n",
      "{'x6', 'x2', 'x7', 'x1', 'x3'}\n",
      "{'x6', 'x2', 'x7', 'x3', 'x9'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x3', 'x0'}\n",
      "{'x5', 'x4', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x4', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x6', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x3', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x4', 'x1', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x4', 'x1', 'x8'}\n",
      "{'x7', 'x4', 'x3', 'x9', 'x0'}\n",
      "{'x7', 'x4', 'x3', 'x9', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x4', 'x3', 'x0'}\n",
      "{'x5', 'x6', 'x1', 'x3', 'x0'}\n",
      "{'x5', 'x6', 'x1', 'x3', 'x0'}\n",
      "{'x6', 'x2', 'x7', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x6', 'x2', 'x7', 'x9', 'x8'}\n",
      "{'x2', 'x7', 'x3', 'x9', 'x0'}\n",
      "{'x2', 'x7', 'x3', 'x9', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x1', 'x9', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x1', 'x9', 'x0'}\n",
      "{'x5', 'x2', 'x4', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x4', 'x0', 'x8'}\n",
      "{'x8', 'x0', 'x6', 'x7'}\n",
      "{'x6', 'x7', 'x3', 'x0', 'x8'}\n",
      "{'x6', 'x1', 'x3', 'x0', 'x8'}\n",
      "{'x0', 'x1', 'x3', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x3', 'x9'}\n",
      "{'x6', 'x9', 'x3', 'x7'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x2', 'x4', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x2', 'x7', 'x3', 'x9'}\n",
      "{'x5', 'x2', 'x7', 'x3', 'x9'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x4', 'x1', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x8'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x3'}\n",
      "{'x5', 'x6', 'x2', 'x3', 'x0'}\n",
      "{'x4', 'x0', 'x6', 'x8'}\n",
      "{'x4', 'x0', 'x6', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x8'}\n",
      "{'x8', 'x6', 'x2', 'x7'}\n",
      "{'x5', 'x6', 'x4', 'x1', 'x3', 'x0'}\n",
      "{'', 'x5', 'x4', 'x1', 'x0'}\n",
      "{'x5', 'x9', 'x2', 'x7'}\n",
      "{'x5', 'x9', 'x2', 'x7'}\n",
      "{'x5', 'x7', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x7', 'x3', 'x9', 'x8'}\n",
      "{'x4', 'x5', 'x6', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x4', 'x0', 'x8'}\n",
      "{'x4', 'x6', 'x9'}\n",
      "{'x4', 'x6', 'x9'}\n",
      "{'x2', 'x7', 'x3', 'x0', 'x8'}\n",
      "{'x2', 'x7', 'x3', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x4', 'x3', 'x9', 'x8'}\n",
      "{'x4', 'x2', 'x3', 'x8'}\n",
      "{'x6', 'x4', 'x3', 'x9', 'x0'}\n",
      "{'x6', 'x4', 'x3', 'x9', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x1', 'x0'}\n",
      "{'x0', 'x6', 'x3'}\n",
      "{'x6', 'x3', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x9', 'x8'}\n",
      "{'x5', 'x6', 'x7', 'x4', 'x8'}\n",
      "{'x6', 'x2', 'x4', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x4', 'x0', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x9', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x9', 'x0'}\n",
      "{'x2', 'x4', 'x1', 'x3', 'x9', 'x0'}\n",
      "{'x2', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x0'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x0'}\n",
      "{'x0', 'x2', 'x8'}\n",
      "{'x2', 'x4', 'x1', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x4', 'x3', 'x8'}\n",
      "{'x5', 'x2', 'x7', 'x4', 'x3'}\n",
      "{'x6', 'x7', 'x4', 'x1', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x7', 'x1', 'x3', 'x0'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x0', 'x8'}\n",
      "{'x5', 'x4', 'x1', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x3', 'x0', 'x8'}\n",
      "{'x5', 'x2', 'x3', 'x0', 'x8'}\n",
      "{'x8', 'x1', 'x9', 'x7'}\n",
      "{'x8', 'x1', 'x7'}\n",
      "{'x4', 'x0', 'x6', 'x2'}\n",
      "{'x4', 'x0', 'x6', 'x2'}\n",
      "{'x5', 'x2', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x4', 'x1', 'x3', 'x9'}\n",
      "{'x5', 'x1', 'x9', 'x0', 'x8'}\n",
      "{'x0', 'x9', 'x5'}\n",
      "{'x6', 'x7', 'x4', 'x1', 'x3', 'x8'}\n",
      "{'x6', 'x7', 'x1', 'x3', 'x8'}\n",
      "{'x2', 'x7', 'x4', 'x3', 'x9', 'x0'}\n",
      "{'x2', 'x7', 'x4', 'x3', 'x9', 'x0', 'x8'}\n",
      "{'x6', 'x2', 'x1', 'x3', 'x8'}\n",
      "{'x6', 'x2', 'x1', 'x3', 'x8'}\n",
      "{'x5', 'x1', 'x3', 'x9', 'x0'}\n",
      "{'x5', 'x1', 'x3', 'x9', 'x0'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x3', 'x9', 'x8'}\n",
      "{'x5', 'x6', 'x2', 'x7', 'x4', 'x3', 'x9', 'x8'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x3', 'x0'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x3', 'x0'}\n",
      "{'x2', 'x7', 'x4', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x2', 'x7', 'x1', 'x3', 'x9', 'x8'}\n",
      "{'x2', 'x4', 'x1', 'x9', 'x0'}\n",
      "{'x2', 'x4', 'x1', 'x3', 'x9', 'x0'}\n",
      "{'x6', 'x2', 'x4', 'x1', 'x3', 'x8'}\n",
      "{'x2', 'x4', 'x1', 'x3', 'x8'}\n",
      "{'x4', 'x6', 'x8'}\n",
      "{'x4', 'x6', 'x8'}\n",
      "{'x6', 'x2', 'x7', 'x4', 'x3', 'x8'}\n",
      "{'x6', 'x7', 'x4', 'x3', 'x0'}\n",
      "{'x6', 'x9', 'x2'}\n",
      "{'x6', 'x9', 'x2'}\n",
      "{'x5', 'x9', 'x8'}\n",
      "{'x5', 'x9', 'x8'}\n"
     ]
    }
   ],
   "source": [
    "preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "targets = [item[\"target\"] for item in test_data[:len(pred_ids)]]\n",
    "\n",
    "is_correct = [check_similarity(target, pred) for (pred, target) in zip(preds, targets)]\n",
    "n_correct = sum(is_correct)\n",
    "\n",
    "# for pred_id in pred_ids:\n",
    "#     pred = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)\n",
    "#     target = test_data[i][\"target\"]\n",
    "#     # break\n",
    "\n",
    "#     # print(output)\n",
    "#     # print(pred)\n",
    "#     # print(target)\n",
    "#     # break\n",
    "#     if check_similarity(target, pred):\n",
    "#         n_correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.156\n"
     ]
    }
   ],
   "source": [
    "print(n_correct / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' x2 [STATE_END] [STATE_START] x2, x3 [STATE_END] [STATE_START] x2, x3, x4 [STATE_END] [STATES_END] [STATES_START] [STATE_START] x2, x3, x4 [STATE_END] [STATES_END] [STATES_START] [STATE_START] x2, x3, x4 [STATE_END] [STATES_END] [STATES_START] [STATES_END] [STATES_START] [STATES_END] [STATES_START] [STATES_END] [STATES_START] [STATES_END] [STATES_START] [STATES_END] [STATES_START] [STATES_END] [STATES_START] [STATES'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/akhare/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 252, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mgenerate(train_tokenized_dataset[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/generation/utils.py:1673\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1657\u001b[0m         input_ids,\n\u001b[1;32m   1658\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1670\u001b[0m     )\n\u001b[1;32m   1671\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1672\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1674\u001b[0m         input_ids,\n\u001b[1;32m   1675\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1676\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1677\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1678\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1679\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1680\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1681\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1682\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1683\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1684\u001b[0m     )\n\u001b[1;32m   1686\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1687\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/generation/utils.py:2531\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2529\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n\u001b[0;32m-> 2531\u001b[0m next_token_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39;49mlogits[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, :]\n\u001b[1;32m   2533\u001b[0m \u001b[39m# pre-process distribution\u001b[39;00m\n\u001b[1;32m   2534\u001b[0m next_tokens_scores \u001b[39m=\u001b[39m logits_processor(input_ids, next_token_logits)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "model.generate(train_tokenized_dataset[0]['input_ids'].to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49msave_model(\u001b[39m\"\u001b[39;49m\u001b[39mgpt2_append_autoreg_str_results\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m output_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mgpt2_append_autoreg_str_results\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfinal_checkpoint\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bseasnet-50-31.cis.upenn.edu/home/akhare/repos/tf_logic/notebooks/string_representation_test_autoreg.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m trainer\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/trainer.py:2843\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2840\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped\u001b[39m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   2842\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[0;32m-> 2843\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save(output_dir)\n\u001b[1;32m   2845\u001b[0m \u001b[39m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   2846\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/trainer.py:2901\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2899\u001b[0m             torch\u001b[39m.\u001b[39msave(state_dict, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   2900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave_pretrained(\n\u001b[1;32m   2902\u001b[0m         output_dir, state_dict\u001b[39m=\u001b[39;49mstate_dict, safe_serialization\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49msave_safetensors\n\u001b[1;32m   2903\u001b[0m     )\n\u001b[1;32m   2905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2906\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/transformers/modeling_utils.py:2187\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[39mfor\u001b[39;00m shard_file, shard \u001b[39min\u001b[39;00m shards\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   2184\u001b[0m     \u001b[39mif\u001b[39;00m safe_serialization:\n\u001b[1;32m   2185\u001b[0m         \u001b[39m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[1;32m   2186\u001b[0m         \u001b[39m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[0;32m-> 2187\u001b[0m         safe_save_file(shard, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(save_directory, shard_file), metadata\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mformat\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[1;32m   2188\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2189\u001b[0m         save_function(shard, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/safetensors/torch.py:281\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_file\u001b[39m(\n\u001b[1;32m    251\u001b[0m     tensors: Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m    252\u001b[0m     filename: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[1;32m    253\u001b[0m     metadata: Optional[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m ):\n\u001b[1;32m    255\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     serialize_file(_flatten(tensors), filename, metadata\u001b[39m=\u001b[39mmetadata)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/safetensors/torch.py:475\u001b[0m, in \u001b[0;36m_flatten\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m failing:\n\u001b[1;32m    467\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    469\u001b[0m \u001b[39m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[39m{\u001b[39;00mfailing\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    473\u001b[0m     )\n\u001b[0;32m--> 475\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    476\u001b[0m     k: {\n\u001b[1;32m    477\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(v\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m: v\u001b[39m.\u001b[39mshape,\n\u001b[1;32m    479\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: _tobytes(v, k),\n\u001b[1;32m    480\u001b[0m     }\n\u001b[1;32m    481\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    482\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/safetensors/torch.py:479\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m failing:\n\u001b[1;32m    467\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    469\u001b[0m \u001b[39m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[39m{\u001b[39;00mfailing\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    473\u001b[0m     )\n\u001b[1;32m    475\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    476\u001b[0m     k: {\n\u001b[1;32m    477\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(v\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m: v\u001b[39m.\u001b[39mshape,\n\u001b[0;32m--> 479\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: _tobytes(v, k),\n\u001b[1;32m    480\u001b[0m     }\n\u001b[1;32m    481\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    482\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/tfl/lib/python3.10/site-packages/safetensors/torch.py:404\u001b[0m, in \u001b[0;36m_tobytes\u001b[0;34m(tensor, name)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    397\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` which is not allowed. It either means you\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms recommended to save\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m pack it before saving.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m tensor\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    403\u001b[0m     \u001b[39m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m     tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    406\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mctypes\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"gpt2_append_autoreg_str_results\")\n",
    "\n",
    "output_dir = os.path.join(\"gpt2_append_autoreg_str_results\", \"final_checkpoint\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
