{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6500994b-716f-433b-a6f0-055781370a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "\n",
    "from models import *\n",
    "from my_datasets import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9df094-e4e3-4738-9da9-145c90fb5432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed80f40-20d7-4389-a0f1-0645c1d2e0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fdebe02-a3e5-4151-ad7d-28b22d7e70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, dataloader, lr_scheduler, tqdm_tick=10):\n",
    "    all_losses, all_accs = [], []\n",
    "    pbar = tqdm(dataloader)\n",
    "    for i, batch in enumerate(pbar):\n",
    "        tokens, labels = batch[\"tokens\"].to(device), batch[\"labels\"].to(device)\n",
    "        out = model(tokens, labels=labels)\n",
    "        loss = out.loss\n",
    "        loss.backward(); optimizer.step(); optimizer.zero_grad(); lr_scheduler.step()\n",
    "\n",
    "        # Track stuff\n",
    "        pred = (out.logits > 0).long()\n",
    "        all_accs.append((pred == labels).float().mean())\n",
    "        all_losses.append(loss.detach().cpu().item())\n",
    "        if (i+1) % tqdm_tick == 0:\n",
    "            avg_loss = torch.tensor(all_losses)[:-tqdm_tick].mean().item()\n",
    "            avg_acc = torch.tensor(all_accs)[:-tqdm_tick].mean().item()\n",
    "            pbar.set_description(\n",
    "                f\"loss {avg_loss:.3f}, acc {avg_acc:.3f}\"\n",
    "            )\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        all_losses: all_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4bd1b7f-5578-4dd7-bc6e-ad5edfc909db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16\n",
    "k = 3\n",
    "model = MyTheoryARModel(n, num_steps=k)\n",
    "model.train().to(device)\n",
    "\n",
    "bsz = 512\n",
    "num_steps = 8192\n",
    "dataset_len = bsz * num_steps\n",
    "dataset = AutoregCustomTokensDataset(n, dataset_len)\n",
    "dataloader = DataLoader(dataset, batch_size=bsz)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "warmup_steps = int(num_steps * 0.1)\n",
    "lr_scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers = [\n",
    "        LinearLR(optimizer, 0.01, 1.0, warmup_steps),\n",
    "        LinearLR(optimizer, 1.0, 0.01, num_steps - warmup_steps),\n",
    "    ],\n",
    "    milestones = [warmup_steps]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffc419-14bb-47e2-8637-780a52751936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cd28f84cab4f06bfde5b5c76594e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonxue/lib/miniconda3/envs/tfl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "ret = train_model(model, optimizer, dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37520e-0fcc-425c-9ebe-baff987aa5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f02a3a-f226-42ec-bec4-2302e29e6249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374626c0-8cae-45b6-8d5c-e65df78b839e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413bc53-8a28-487c-a207-676eab398c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
